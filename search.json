[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Curiosity Trace",
    "section": "",
    "text": "Fine-tuning vs Prompting\n\n\n\n\n\n\nNLP\n\n\nLLM\n\n\n\n\n\n\n\n\n\nFeb 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nStructured output quirks\n\n\n\n\n\n\nllm\n\n\n\n\n\n\n\n\n\nFeb 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nIs Everything an Abstraction?\n\n\n\n\n\n\nPhilosophy\n\n\nScience\n\n\n\n\n\n\n\n\n\nJan 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBenchmark CUDA kernel with Python\n\n\n\n\n\n\ncuda\n\n\npython\n\n\n\n\n\n\n\n\n\nMar 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nServing open source LLM as OpenAI API\n\n\n\n\n\n\nllm\n\n\nhuggingface\n\n\nopenai\n\n\nlitellm\n\n\n\n\n\n\n\n\n\nJan 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHow to use HuggingFace datasets with fastai?\n\n\n\n\n\n\nfastai\n\n\nhuggingface\n\n\ndatasets\n\n\n\n\n\n\n\n\n\nApr 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nHow to use a custom model with fastai cnn_learner?\n\n\n\n\n\n\nfastai\n\n\n\n\n\n\n\n\n\nFeb 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nHow many coin flips does high confidence need?\n\n\n\n\n\n\nmath\n\n\n\n\n\n\n\n\n\nJul 25, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nWhy is initialization important in neural networks?\n\n\n\n\n\n\nneural-network\n\n\n\n\n\n\n\n\n\nJun 25, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nFourier Transform\n\n\n\n\n\n\nmath\n\n\n\nA minimal introduction\n\n\n\n\n\nJun 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nHalting problem\n\n\n\n\n\n\ncomputer-science\n\n\n\n\n\n\n\n\n\nApr 7, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/fine-tuning-vs-prompting/index.html",
    "href": "posts/fine-tuning-vs-prompting/index.html",
    "title": "Fine-tuning vs Prompting",
    "section": "",
    "text": "Large Language Models (LLMs) have become incredibly powerful, but they often need adaptation to perform specific tasks effectively. While fine-tuning model weights was traditionally the default approach, the massive size of modern LLMs makes it less practical. With LLMs’ strong general capabilities, prompt engineering has become a preferred method for adaptation. But how do these approaches compare in practice?\nThis post explores this question through a case study on joint entity-relation extraction, a fundamental NLP task used in constructing knowledge graphs. The task involves identifying both entities and their relationships from unstructured text.\nConsider the following input text:\n“Buzz Aldrin was born in Glen Ridge, New Jersey on January 20th, 1930.”\nThe extracted triples could be:\n\nBuzz Aldrin | birth place | Glen Ridge, New Jersey\nBuzz Aldrin | birth date | January 20th, 1930"
  },
  {
    "objectID": "posts/fine-tuning-vs-prompting/index.html#problem",
    "href": "posts/fine-tuning-vs-prompting/index.html#problem",
    "title": "Fine-tuning vs Prompting",
    "section": "",
    "text": "Large Language Models (LLMs) have become incredibly powerful, but they often need adaptation to perform specific tasks effectively. While fine-tuning model weights was traditionally the default approach, the massive size of modern LLMs makes it less practical. With LLMs’ strong general capabilities, prompt engineering has become a preferred method for adaptation. But how do these approaches compare in practice?\nThis post explores this question through a case study on joint entity-relation extraction, a fundamental NLP task used in constructing knowledge graphs. The task involves identifying both entities and their relationships from unstructured text.\nConsider the following input text:\n“Buzz Aldrin was born in Glen Ridge, New Jersey on January 20th, 1930.”\nThe extracted triples could be:\n\nBuzz Aldrin | birth place | Glen Ridge, New Jersey\nBuzz Aldrin | birth date | January 20th, 1930"
  },
  {
    "objectID": "posts/fine-tuning-vs-prompting/index.html#method-and-experimental-setup",
    "href": "posts/fine-tuning-vs-prompting/index.html#method-and-experimental-setup",
    "title": "Fine-tuning vs Prompting",
    "section": "Method and Experimental Setup",
    "text": "Method and Experimental Setup\nFor the analysis, the WebNLG dataset was used, where each example consists of a natural language text and its corresponding entity-relation triples. The dataset was preprocessed by converting relations to plain English (e.g., “birthPlace” to “birth place”) and randomly concatenated 1–7 samples to create texts of varying lengths. This approach was intended to simulate real-world documents where context and content vary in length and complexity. However, it is acknowledged that this method may not fully capture the natural coherence and structural characteristics of actual documents. A conversation format version with user-assistant message pairs was also created for fine-tuning. The preprocessed dataset is available on Hugging Face. The train split was used for model adaptation and the dev split for benchmarking.\nExperiments were conducted using Llama-3 8B, comparing fine-tuning and prompting approaches.\nFor fine-tuning, LoRA (Hu et al. 2021) with rank 64 was used to efficiently adapt the model weights using varying amounts of training data (100 to 8,870 examples). The configuration included:\n\nLearning rate: 1.0e-4\nBatch size: 8\nTraining epochs: 1\nScheduler: cosine with 0.1 warmup ratio\nMixed precision: bf16\n\nFor prompt optimization, the DSPy library (Khattab et al. 2024) was used to optimize prompts with different numbers of few-shot examples (0 to 24). All experiments used greedy decoding (temperature=0.0) to maintain consistency and reproducibility.\nPerformance was evaluated using three key metrics:\n\nExact Match F1: This measures the model’s ability to extract triples that exactly match the ground truth, considering both precision and recall.\nFuzzy Match F1: This more lenient metric allows for minor variations in entity boundaries and relation phrases while still capturing the core semantic relationships.\nPairwise Comparisons: Qwen-2.5-32B was used to compare outputs from different models with LLM-as-a-judge technique (Zheng et al. 2023). For each test example, the judge was presented with:\n\nThe input text\nTwo competing model outputs (triples, with randomized order to eliminate position bias)\nA rubric emphasizing factual correctness, completeness, and precision\n\n\nThe judge was instructed to evaluate the triples based on key criteria including completeness of relationship extraction, factual accuracy, strict alignment with source text, precision without overgeneration, absence of redundancy, and informative representation*. For each comparison, the judge provided a concise explanation of its reasoning before making a final decision of A, B, or DRAW. These decisions were then aggregated into ELO ratings, providing a more nuanced evaluation of output quality beyond string matching. This comparison was applied to 100 samples randomly chosen from the dev split."
  },
  {
    "objectID": "posts/fine-tuning-vs-prompting/index.html#results",
    "href": "posts/fine-tuning-vs-prompting/index.html#results",
    "title": "Fine-tuning vs Prompting",
    "section": "Results",
    "text": "Results\nThe evaluation used both traditional metrics (F1 scores) and pairwise comparisons judged by another LLM. Here are the key results:\nTable 1: Comparison of fine-tuning and prompting approaches across different metrics.\n\n\n\n\n\n\n\n\n\n\n\nAdaptation Method\nNum Samples\nOptimizer\nExact F1\nFuzzy F1\nELO Rating\n\n\n\n\nFine-tuning\n8,870\nAdamW\n0.94\n0.98\n1502.4\n\n\nFine-tuning\n1,000\nAdamW\n0.81\n0.93\n1506.5\n\n\nFine-tuning\n100\nAdamW\n0.62\n0.84\n1448.4\n\n\nFine-tuning\n16\nAdamW\n0.07\n0.24\n1430.9\n\n\nPrompting\n24\nBFSRS-High\n0.32\n0.69\n1507.8\n\n\nPrompting\n8\nMIPROv2-Medium\n0.24\n0.63\n1510.6\n\n\nPrompting\n0\nNone\n0.02\n0.34\n1472.4\n\n\n\n\nFigure 1: The impact of the number of few-shot examples on the F1 score.\nThe plot demonstrates that F1 scores initially improve rapidly with additional few-shot examples but begin to plateau after around 16 examples. This saturation effect suggests there may be limited benefit to including more than 16 examples in the prompt, as the marginal gains diminish.\nAnalysis of these results reveals several key patterns:\n\nFine-tuning demonstrates strong but nuanced performance:\n\nPerformance scales clearly with data, improving from 0.07 to 0.94 Exact F1 score as shown in Table 1\nThe full dataset (8,870 examples) achieves the best F1 results\nWith very limited data (16 examples), fine-tuning performs poorly\nBut with reasonable amount of data (100 examples), the model produces reasonable results\nInterestingly, using 1,000 examples led to higher ELO ratings than using 8,870 examples, suggesting possible overfitting to the dataset\nThis indicates that beyond a certain point, additional training data may lead to diminishing returns\n\n\n\n\n\nPrompt optimization demonstrates remarkable efficiency:\n\nUsing MIPROv2-Medium with just 8 examples achieved the highest ELO rating (1510.6) despite lower F1 scores\nF1 scores improve rapidly with additional examples but plateau around 16 examples\nThese results indicate that prompt optimization may be more sample-efficient for achieving real-world quality\nThe approach requires significantly less computational resources compared to fine-tuning\n\n\n\n\n\nZero-shot prompting underscores adaptation importance:\n\nWithout any examples, performance is poor (0.02 Exact F1, 0.34 Fuzzy F1)\nZero-shot approach yields the lowest performance across all metrics (ELO rating 1472.4)\nAdding even a few examples leads to dramatic improvements, as illustrated in Figure 1\nThis clearly demonstrates that task-specific guidance is crucial for effective performance\n\n\n\nUnderstanding Metric Discrepancy\nThe gap between classical metrics (F1 scores) and ELO ratings reveals important insights about evaluation:\n\nClassical metrics like F1 scores measure exact matching against ground truth, which may be overly strict:\n\nMinor variations in phrasing that preserve meaning are penalized\nAlternative but valid entity boundaries may reduce scores\nSemantically equivalent relationships expressed differently lower F1 scores\n\n\n\n\n\nELO ratings from the judge model capture more nuanced aspects:\n\nFactual correctness and alignment with source text\nCompleteness of relationship extraction\nPrecision without redundancy or overgeneration\nOverall quality and usefulness of the extracted information\n\n\nThis explains why prompt optimization can achieve high ELO ratings despite lower F1 scores - it may produce high-quality outputs that differ slightly from the exact ground truth format.\n\n\nPractical Implementation Considerations\nThe choice between fine-tuning and prompt optimization involves several trade-offs:\n\nComputational Resources:\n\nFine-tuning requires significant compute for training and access to the model weights\nPrompt optimization needs only inference resources; hence, it is applicable to proprietary models as well\n\n\n\n\n\nDeployment Flexibility:\n\nPrompts can be modified quickly in production\nFine-tuned models require redeployment\nIteration speed varies significantly\n\n\n\n\n\nData Requirements:\n\nFine-tuning needs large datasets for best results\nPrompt optimization works well with limited examples\nData collection and preparation costs differ\n\n\n\n\n\nMaintenance:\n\nPrompts are easier to version control and update\nFine-tuned models require careful weight management\nDebugging complexity varies between approaches"
  },
  {
    "objectID": "posts/fine-tuning-vs-prompting/index.html#limitations-and-future-work",
    "href": "posts/fine-tuning-vs-prompting/index.html#limitations-and-future-work",
    "title": "Fine-tuning vs Prompting",
    "section": "Limitations and Future Work",
    "text": "Limitations and Future Work\nThe analysis has several limitations that point to interesting future directions:\n\nLLM-as-a-Judge Limitations:\n\nA 32B parameter model was used for pairwise comparisons. Using larger models like Llama-70B or GPT-4o could provide more reliable evaluations.\nThe evaluation prompt could be further optimized. For instance, self-consistency prompting (Wang, Xuezhi et al. 2022) could be employed to improve the quality of the judge LLM.\n\n\n\n\n\nTask and Dataset Coverage:\n\nResults are specific to entity-relation extraction. Testing on other NLP tasks (classification, Q&A) would provide broader insights.\nAdditional datasets would help validate findings.\n\n\n\n\n\nStatistical Significance of Results:\n\nThe differences in ELO ratings and F1 scores across configurations are relatively small.\nFuture work should include statistical significance testing to determine if these differences are meaningful."
  },
  {
    "objectID": "posts/fine-tuning-vs-prompting/index.html#conclusion",
    "href": "posts/fine-tuning-vs-prompting/index.html#conclusion",
    "title": "Fine-tuning vs Prompting",
    "section": "Conclusion",
    "text": "Conclusion\nWhile fine-tuning achieves higher F1 scores, prompt optimization can be surprisingly effective when properly implemented. The choice between these approaches depends on specific constraints and requirements:\n\nIf sufficient training data and computational resources are available, fine-tuning provides robust performance\nIf data or compute is limited, optimized prompting can offer a viable alternative\nWhen rapid iteration or deployment flexibility is crucial, prompt optimization has clear advantages\nIn cases where exact matching is critical, fine-tuning may be preferable\n\n\nNote:\nThe gap between traditional metrics and LLM-based evaluation suggests that while quantitative scores provide one perspective, qualitative assessment might be necessary to understanding real-world performance.\n\nThe code for the experiments and analysis is available at this GitHub repository."
  },
  {
    "objectID": "posts/fine-tuning-vs-prompting/index.html#references",
    "href": "posts/fine-tuning-vs-prompting/index.html#references",
    "title": "Fine-tuning vs Prompting",
    "section": "References",
    "text": "References\n\nKhattab, Omar and Singhvi, Arnav and Maheshwari, Paridhi and Zhang, Zhiyuan and Santhanam, Keshav and Vardhamanan, Sri and Haq, Saiful and Sharma, Ashutosh and Joshi, Thomas T. and Moazam, Hanna and Miller, Heather and Zaharia, Matei and Potts, Christopher. (2024). DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines. In The Twelfth International Conference on Learning Representations.\nWebNLG\nLlama-3 8B\nQwen-2.5-32B\nHu, J.E., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., & Chen, W. (2021). LoRA: Low-Rank Adaptation of Large Language Models. ArXiv, abs/2106.09685.\nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E.H., & Zhou, D. (2022). Self-Consistency Improves Chain of Thought Reasoning in Language Models. ArXiv, abs/2203.11171.\nZheng, L., Chiang, W., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E.P., Zhang, H., Gonzalez, J.E., & Stoica, I. (2023). Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. ArXiv, abs/2306.05685.\n\n\n\n\nAppendix\n\n\nExample Comparisons\nTo further illustrate the differences between fine-tuning and prompt optimization, I present a few selected examples from my model comparison, where A and B denote best performing fine-tuned and prompt optimized models respectively:\n\nExample 1: Decision was A\nText:\nCornell University is in Ithaca, New York and their nickname is Cornell Big Red. They are the publisher of Administrative Science Quarterly and are affiliated with the Association of American Universities.\nTriples A:\n- “Administrative Science Quarterly | publisher | Cornell University” - “Cornell University | nickname | Cornell Big Red” - “Cornell University | affiliation | Association of American Universities” - “Cornell University | city | Ithaca, New York”\nTriples B:\n- “Cornell University | city | Ithaca, New York” - “Cornell University | nickname | Cornell Big Red” - “Cornell University | academic publisher | Administrative Science Quarterly” - “Cornell University | affiliation | Association of American Universities”\nExplanation:\nSet A correctly identifies ‘Cornell University’ as the publisher of ‘Administrative Science Quarterly’, whereas Set B incorrectly reverses this relationship. Therefore, Set A is more correct and complete.\n\n\nExample 2: Decision was B\nText:\nAlpena County Regional Airport runway name is 1/19. Swords is a part of the Dublin European Parliamentary constituency. The United States (with a leader, John Roberts) boasts the Bacon Explosion. American Abraham A. Ribicoff was born in New Britain, Connecticut and is married to Ruth Ribicoff. One of the ethnic groups in the U.S. are Native Americans. Georgia is in the country of United States. Atlanta; has a total area of 347.1 square kilometres, a population density of 1299.0 inhabitants per square kilometre, and the area codes: 404, 678, and 470. The birthplace of Alfred Garth Jones is Manchester, (part of Greater Manchester), England. In Manchester the Labour Party is in the majority.\nTriples A:\n- “Alpena County Regional Airport | runway name | 1/19” - “Swords, Dublin | is part of | Dublin (European Parliament constituency)” - “Bacon Explosion | country | United States” - “United States | leader | John Roberts” - “Abraham A. Ribicoff | birth place | New Britain, Connecticut” - “Abraham A. Ribicoff | spouse | Ruth Ribicoff” - “United States | ethnic group | Native Americans in the United States” - “Abraham A. Ribicoff | nationality | American” - “Georgia (U.S. state) | country | United States” - “Atlanta | area code | 404, 678, 470” - “Atlanta | population density | 1299.0 (inhabitants per square kilometre)” - “Atlanta | area total | 347.1 (square kilometres)” - “Alfred Garth Jones | birth place | England” - “Alfred Garth Jones | birth place | Manchester” - “Manchester | is part of | Greater Manchester” - “Manchester | leader | Labour Party (UK)”\nTriples B:\n- “Alpena County Regional Airport | runway name | 1/19” - “Swords | is part of | Dublin (European Parliament constituency)” - “United States | leader | John Roberts” - “United States | dish variation | Bacon Explosion” - “Abraham A. Ribicoff | spouse | Ruth Ribicoff” - “Abraham A. Ribicoff | birth place | New Britain, Connecticut” - “United States | ethnic group | Native Americans in the United States” - “Georgia (U.S. state) | country | United States” - “Atlanta | area code | 404, 678, 470” - “Atlanta | population density | 1299.0 (inhabitants per square kilometre)” - “Atlanta | total area | 347.1 (square kilometres)” - “Alfred Garth Jones | birth place | Manchester, England” - “Manchester | has to its majority | Labour Party” - “Manchester | is part of | Greater Manchester”\nExplanation:\nModel B’s triples are complete, correct, and consistent with the text. It correctly states the birth place of Alfred Garth Jones as ‘Manchester, England’ and avoids the duplication issue. Therefore, Model B’s triples are better.\n\n\nExample 3: Decision was DRAW\nText:\nThe ISBN number of A Long Long Way is 0-670-03380-4.\nTriples A:\n- “A Long Long Way | isbn number | 0-670-03380-4”\nTriples B:\n- “A Long Long Way | isbn number | 0-670-03380-4”\nExplanation:\nBoth model A and model B extract the same triple, which is complete, correct, consistent with the text, and does not contain duplicates. Therefore, both models perform equally well."
  },
  {
    "objectID": "posts/tgi-litellm-openai/index.html",
    "href": "posts/tgi-litellm-openai/index.html",
    "title": "Serving open source LLM as OpenAI API",
    "section": "",
    "text": "In this tutorial, we’ll see how to serve an open source language model with OpenAI compatible API using HuggingFace Text Generation Inference and LiteLLM OpenAI proxy server. This enables us to use any tool compatible with OpenAI API.\nWe’ll use 4-bit quantized Llama-2 7B chat model to lower GPU memory requirement. Launch TGI server on a device with GPU:\n#!/bin/bash\n\nVOLUME=\"${HOME}/.cache/huggingface/tgi\"\nmkdir -p $VOLUME\n\ndocker run --gpus all --shm-size 1g \\\n    -p 8080:80 \\\n    -v \"${VOLUME}\":/data \\\n    ghcr.io/huggingface/text-generation-inference:latest \\\n     --trust-remote-code \\\n     --model-id \"NousResearch/llama-2-7b-chat-hf\" \\\n     --quantize bitsandbytes-nf4 \\\n     --dtype float16\nHuggingFace Text Generation Inference server supports only text completion. However, we want to send chat messages with system, user, and assistant roles same as OpenAI models (i.e. ChatML format). Fortunately, LiteLLM supports Llama-2 chat template, which will convert chat messages to text prompt before calling TGI server. All we need to do is to specify model paramater as huggingface/meta-llama/Llama-2-7b. For models not supported by LiteLLM, we can create a custom template.\nHere is the config for LiteLLM OpenAI proxy Server.\n# config.yaml\nmodel_list:\n  - model_name: llama-2-7b-chat # arbitrary alias for our mdoel\n    litellm_params: # actual params for litellm.completion()\n      model: \"huggingface/meta-llama/Llama-2-7b\"\n      api_base: \"http://localhost:8080/\"\n      max_tokens: 1024\n\nlitellm_settings:\n  set_verbose: True\nLaunch LiteLLM OpenAI proxy server:\nlitellm --config config.yaml\nSet API base url.\n\nAPI_BASE=\"http://localhost:8000/\"\n\nLet’s use the model with completion function provided by litellm library, first.\n\nfrom litellm import completion \n\nmessages = [\n    {\"content\": \"You are helpful assistant.\",\"role\": \"system\"},\n    {\"content\": \"Tell me 3 reasons to live in Istanbul.\",\"role\": \"user\"},\n]\n\nresponse = completion(\n  api_base=API_BASE,\n  model=\"llama-2-7b-chat\", \n  custom_llm_provider=\"openai\", # so that messages are sent to proxy server as they are\n  messages=messages, \n  temperature=0.3,\n)\n\nprint(response.choices[0].message.content)\n\n\nIstanbul is a city with a rich history and culture, and there are many reasons to live there. Here are three:\n\n1. Cultural Diversity: Istanbul is a city that straddles two continents, Europe and Asia, and has a unique cultural identity that reflects its history as a crossroads of civilizations. The city is home to a diverse population, including Turks, Kurds, Greeks, Armenians, and other ethnic groups, each with their own traditions and customs. This diversity is reflected in the city's architecture, food, music, and art, making Istanbul a vibrant and exciting place to live.\n2. Historical Landmarks: Istanbul is home to some of the most impressive historical landmarks in the world, including the Hagia Sophia, the Blue Mosque, and the Topkapi Palace. These landmarks are not only important cultural and religious sites, but also serve as a reminder of the city's rich history and its role in the development of civilizations. Living in Istanbul, you are surrounded by these incredible structures, which are a source of inspiration and pride for the city's residents.\n3. Gastronomy: Istanbul is known for its delicious and diverse food scene, which reflects the city's cultural diversity. From traditional Turkish dishes like kebabs and baklava, to Greek and Middle Eastern cuisine, there is something for every taste and budget. Living in Istanbul, you have access to a wide range of fresh produce, spices, and other ingredients, which are used to create mouth-watering dishes that are both healthy and delicious.\n\nOverall, Istanbul is a city that offers a unique and enriching experience for those who live there. Its cultural diversity, historical landmarks, and gastronomy make it a vibrant and exciting place to call home.\n\n\nNow, let’s use the model with llama-index library. The subtle point is that LiteLLM class in llama-index expects custom_llm_provider parameter in additional_kwargs argument.\n\nfrom llama_index.llms import ChatMessage, LiteLLM\n\nllm = LiteLLM(\n    api_base=API_BASE,\n    api_key=\"\",\n    model=\"llama-2-7b-chat\", \n    temperature=0.3,\n    additional_kwargs=dict(\n        custom_llm_provider=\"openai\", # so that messages are sent to proxy server as they are\n    ),\n)\n\nmessages = [\n    ChatMessage.parse_obj({\"content\": \"You are helpful assistant.\", \"role\": \"system\"}),\n    ChatMessage.parse_obj({\"content\": \"Tell me 3 reasons to live in London.\", \"role\": \"user\"}),\n]\nresponse = llm.chat(messages) \nprint(response.message.content)\n\n\n\n1. Cultural diversity: London is a melting pot of cultures, with people from all over the world calling it home. This diversity is reflected in the city's food, art, music, and fashion, making it a vibrant and exciting place to live.\n2. World-class amenities: London has some of the best amenities in the world, including top-notch restaurants, theaters, museums, and sports venues. Whether you're looking for a night out on the town or a quiet evening at home, London has something for everyone.\n3. Investment opportunities: London is a major financial hub, with many opportunities for investment in real estate, business, and other industries. The city's strong economy and stable political environment make it an attractive place to invest and grow your wealth.\n\n\n\nmessages = [\n    ChatMessage(content=\"You are an hilarious comedian who is famous with their sarcastic jokes.\", role=\"system\"),\n    ChatMessage(content=\"Tell me a joke about front-end developers.\", role=\"user\"),\n]\nresponse = llm.chat(messages) \nprint(response.message.content)\n\n\n\nI'm glad you think I'm hilarious! Here's a joke for you:\n\nWhy did the front-end developer break up with his girlfriend?\n\nBecause he wanted a more responsive relationship! 😂\n\nI hope you found that one amusing! Front-end developers can be a funny topic, but I'm sure they won't mind a good-natured jab or two. Let me know if you want another one!"
  },
  {
    "objectID": "posts/confidence-intervals/index.html",
    "href": "posts/confidence-intervals/index.html",
    "title": "How many coin flips does high confidence need?",
    "section": "",
    "text": "Code\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport seaborn\n\n# Set seaborn aesthetic parameters to defaults\nseaborn.set()\nseaborn.set_style('whitegrid')\nseaborn.set_palette(\"dark\")\n\nimport math\nimport numpy as np\nfrom scipy.special import comb\n\nnp.random.seed(42)\nYou walked into a casino, there is a game of tossing coins. You don’t know whether the coin is biased or fair. But, you can observe the game and keep the stats. How many tosses would you observe to measure the probability of the coin?\nThough it is not possible to exactly determine the underlying probability distribution of the coin, we can measure in high confidence, if we observe enough tosses.\nThe probability distribution of a coin toss is known as Bernoulli distribution parameterized with p, the probability of heads. There is no way we can directly measure this. Instead, we count the heads, \\(k\\), and divide it into total number of tosses, \\(n\\). How do we know that this is a good way of measuring p? Well, the probability distribution of number of heads is Binomial distribution, parameterized with p and n. And the expectation of Bernoulli distribution is \\(np\\). Therefore, if we count heads and divide it by total number of tosses, we have an estimate of \\(p\\).\n\\[\n\\text{Binomial Distribution } \\\\\n\\]\n\\[\nP_{p,n}(k: \\text{\\#heads}) =  \\binom{N}{k} \\cdot p^k \\cdot (1-p)^{n-k}\n\\]\n\\[\n\\mathbb{E}[\\#heads]= \\sum\\limits_{i=1}^n  \\mathbb{E}[\\mathbb{1}[coin_i]] =\n\\sum\\limits_{i=1}^n  p = np  \\tag*{(Linearity of Expectation) }\n\\]\n\\[\n\\mathbb{1}[coin_i] = 1   \\hspace{35pt}\\text{if ith coin comes heads}\\\\\n0 \\hspace{48pt} \\text{otherwise} \\\\\n\\]\nLet’s simulate this for 100 tosses. First, let’s randomly pick \\(p\\) and see what our estimation ends up.\nn = 100\np = np.random.rand(1)[0]\ntosses = np.random.rand(n)\nheads = tosses &lt; p\n\nspace = np.array(['Heads', 'Tails'])\nprint(space[heads.astype(np.int)])\nprint()\n\np_est = heads.sum() / n\nerror = abs(p - p_est)\nprint(f\"True p\\t\\t: {p}\")\nprint(f\"Estimated p\\t: {p_est}\")\nprint(f\"Error\\t\\t: {error:%}\")\n\n['Heads' 'Heads' 'Heads' 'Tails' 'Tails' 'Tails' 'Heads' 'Heads' 'Heads'\n 'Tails' 'Heads' 'Heads' 'Tails' 'Tails' 'Tails' 'Tails' 'Heads' 'Heads'\n 'Tails' 'Heads' 'Tails' 'Tails' 'Tails' 'Heads' 'Heads' 'Tails' 'Heads'\n 'Heads' 'Tails' 'Heads' 'Tails' 'Tails' 'Heads' 'Heads' 'Heads' 'Tails'\n 'Tails' 'Heads' 'Heads' 'Tails' 'Heads' 'Tails' 'Heads' 'Tails' 'Heads'\n 'Tails' 'Heads' 'Heads' 'Tails' 'Heads' 'Heads' 'Heads' 'Heads' 'Heads'\n 'Heads' 'Tails' 'Tails' 'Tails' 'Tails' 'Heads' 'Tails' 'Heads' 'Tails'\n 'Tails' 'Heads' 'Tails' 'Heads' 'Tails' 'Heads' 'Heads' 'Tails' 'Tails'\n 'Heads' 'Heads' 'Heads' 'Heads' 'Tails' 'Tails' 'Tails' 'Heads' 'Heads'\n 'Tails' 'Tails' 'Tails' 'Tails' 'Heads' 'Heads' 'Heads' 'Heads' 'Tails'\n 'Heads' 'Heads' 'Heads' 'Heads' 'Heads' 'Heads' 'Heads' 'Tails' 'Tails'\n 'Tails']\n\nTrue p      : 0.3745401188473625\nEstimated p : 0.45\nError       : 7.545988%\n7% error! Way too much.\nLet’s repeat this experiment for different sample sizes and see how the error changes.\ndef measure(p, n):\n    tosses = np.random.rand(n)\n    heads = tosses &lt; p\n    p_est = heads.sum() / n\n    return p_est\nsizes = np.array([2**i for i in range(1, 15)])\nestimates = np.array([measure(p, int(size)) for size in sizes])\nerrors = np.abs(estimates - p)\nplt.figure()\nplt.scatter(sizes, errors)\nplt.xscale('log')\nThough there are some deviations, it seem’s like error decreases as the sample size grows. But, how to know where to stop? Also, how confident are we about our measurements considering the deviations in them? Fortunately, with the help of probability theory, we can determine the number of samples required to reach the desired confidence level for our estimate."
  },
  {
    "objectID": "posts/confidence-intervals/index.html#confidence-intervals",
    "href": "posts/confidence-intervals/index.html#confidence-intervals",
    "title": "How many coin flips does high confidence need?",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nWe want to ensure that our estimate is close to true value of \\(p\\). In other words, we want the probability of the event that our estimate is within a small interval around \\(p\\) as high as possible. This probability, a.k.a. confidence level, can be formulated as such,\n\\[\n\\text{confidence level} = P[|p_{est} - p| \\leq \\text{interval}] \\\\\n= 1 - P[|p_{est} - p| &gt; \\text{interval}]\n= 1 - P[p_{est} &gt; p + \\text{interval} \\hspace{4pt} \\cup \\hspace{4pt} p_{est} &lt; p - \\text{interval}] \\\\\n= 1 - P[\\frac{k}{n} &gt; p + \\text{interval} \\hspace{4pt} \\cup \\hspace{4pt} \\frac{k}{n} &lt; p - \\text{interval}] \\\\\n= 1 - (P[k &gt; n (p + \\text{interval})] + P[k &lt; n (p - \\text{interval})])\n\\]\n\\[\nP[k &gt; n (p + \\text{interval})] = P[n-k &lt; n - n (p + \\text{interval})] = P[n-k &lt; n (1 - p - \\text{interval}))]\n\\]\n\\[\n\\text{confidence level} = 1 - (P[n-k &lt; n (1 - p - \\text{interval}))] + P[k &lt; n (p - \\text{interval})]) \\\\\n= 1 - ( F_{n,1-p}[n(1 - p - \\text{interval})] + F_{n,p}[n(p - \\text{interval})] ) \\\\\nF: \\text{Cumulative Distribution Function}\n\\]\nSo, we need to calculate cumulative mass function (CMF) of Binomial distribution parameterized with \\(p\\) and \\(n\\). But, we don’t know p. It turns out that the above expression takes it minimum value at \\(p=0.5\\). Therefore, we can find a lower bound on confidence level with \\(p=0.5\\). The formula for confidence level becomes,\n\\[\n\\text{confidence level} &gt;= 1 - ( F_{n,0.5}[n(0.5 - \\text{interval})] + F_{n,0.5}[n(0.5 - \\text{interval})] ) \\\\\n= 1 - 2 F_{n,0.5}[n(0.5 - \\text{interval})]\n\\]\nSince we know the probability mass function (PMF) of Binomial Distribution, it is easy to calculate CMF.\n\\[\nF_x[X] = P_x[x &lt; X] = \\sum\\limits_{i=0}^{X-1} P[x=i]\n\\]\nLet’s define PMF by using scipy.special.comb for combination function.\n\nfrom scipy.special import comb\n\n\ndef binomial_pmf_naive(n, p, k):\n    return comb(n, k) * p ** k * (1-p)**(n-k)\n\n\nbinomial_pmf_naive(2, 0.5, 0)\n\n0.25\n\n\n\nbinomial_pmf_naive(100, .5, 50)\n\n0.07958923738717873\n\n\n\nbinomial_pmf_naive(2000, .5, 1000)\n\ninf\n\n\nAs you see, for big n values, Scipy’s comb method overflows since it is out of bounds of Python’s floating point number. To overcome this, we can use an approximation for log of combination function.\n\\[\n\\binom{N}{k} \\sim \\frac{2^{nH(k/n)}}{\\sqrt{2\\pi k (1 - \\frac{k}{n})}}\n\\] \\[\nH(a) = a \\log \\frac{1}{a} + (1-a) \\log \\frac{1}{1-a} \\tag*{[Entropy function]}\n\\]\n\ndef entropy_function(a, eps=1e-32):\n    return a * np.log2(1 / (a + eps)) + (1 - a) * np.log2(1 / (1 - a + eps))\n\n\n\nCode\nx = np.linspace(0, 1, 100)\ny = entropy_function(x) \n\nfig, ax = plt.subplots()\nax.plot(x, y)\nax.set_title(\"Entropy function\");\n\n\n\n\n\n\n\n\n\n\n@np.vectorize\ndef log_comb_approximate(n, k, eps=1e-32):\n    if k==0 or k==n: return float(\"-inf\")\n    if k==1 or k==(n-1): return np.log(n)\n    \n    a = k/n\n    return n * entropy_function(a) * np.log(2) - np.log(np.sqrt(2 * np.pi * a * (1-a) * n) + eps)\n\n\n\nCode\nn = 1000\nplt.figure(figsize=(8, 4))\nplt.plot(comb(n, np.arange(n+1)), color='blue')\nplt.plot(np.exp(log_comb_approximate(n, np.arange(n+1))).T, color='red')\nplt.legend(['scipy comb', 'approx. comb']);\n\n\n\n\n\n\n\n\n\n\n\nCode\nn = 100\nk = np.arange(n)\nexpected = comb(n, k)\napproximate = np.exp(log_comb_approximate(n, k))\n\neps = 1e-8\nrel_error = np.abs(approximate/(expected+eps) - 1)\n\nfig, ax = plt.subplots(figsize=(8, 4))\nax.semilogy(k, rel_error)\nax.set_title('Combination function approximation error');\n\n\n\n\n\n\n\n\n\nOK, the approximation seems pretty close. Now, we can define PMF.\n\ndef binomial_pmf(n, p, k, eps=1e-32):\n    if isinstance(p, (float, int)):\n        p = [p]\n    if isinstance(k, (float, int)):\n        k = [k]\n    p = np.array(p)[:, None]\n    k = np.array(k)[None, :]\n    return np.exp(log_comb_approximate(n, k) + k * np.log(p + eps) + (n - k) * np.log(1 - p + eps))\n\n\n\nCode\nn = 2000\np = 0.4\nk = np.arange(n+1)\npmf = binomial_pmf(n, p, k)\n\nplt.figure(figsize=(12, 6))\nplt.plot(k, pmf.T)\nplt.xlabel(\"Number of heads\")\nplt.ylabel(\"Probability Mass Function\")\nplt.title(f\"Binomial Distribution with p={p}\");\n\n\n\n\n\n\n\n\n\nLet’s sanity-check approximated PMF.\n\nprint(pmf.sum())\n\n1.0001320546143024\n\n\nCMF is just cumulative sum of PMF.\n\n\nCode\ncmf = pmf.cumsum(-1)\n\nplt.figure(figsize=(12, 6))\nplt.plot(k, cmf.T)\nplt.xlabel(\"Number of heads\")\nplt.ylabel(\"Cumulative Mass Function\")\nplt.title(f\"Binomial Distribution with p={p}\");\n\n\n\n\n\n\n\n\n\nNow, let’s define a function that calculates confidence level for given sample size and interval.\n\\[\\text{confidence level} \\geq 1 - 2 F_{n,0.5}[n(0.5 - \\text{interval})]\\]\n\ndef compute_confidence(n, interval):\n    p = 1/2\n    low = np.ceil(n*(p-interval)).astype(np.int)\n    pmf = binomial_pmf(n, p, k=np.arange(low+1)).squeeze()\n    cmf = pmf.cumsum()\n    return 1 - 2 * cmf[low]\n\nLet’s say we want a tight interval around true value of \\(p\\), so we set it 0.01.\n\ninterval = 0.01\n\nAnd for sample size of \\(n=100\\), the minimum confidence becomes,\n\nn = 100\nconf = compute_confidence(n, interval)\nprint(f\"confidence level &gt;{conf: %}\")\n\nconfidence level &gt; 7.725091%\n\n\nThis is very low, we definitely need more samples. Let’s go for \\(n=1000\\).\n\nn=1000\nconf = compute_confidence(n, interval)\nprint(f\"confidence level &gt;{conf: %}\")\n\nconfidence level &gt; 45.188993%\n\n\nIt gets better, but we cannot keep trying all values one by one. Let’s plot confidence level against different sample sizes.\n\n\nCode\nimport matplotlib.ticker as mtick\n\nns = np.power(2, np.arange(6, 18)) # sample sizes\nconfs = np.array([compute_confidence(n, interval) for n in ns]).squeeze()\n\nplt.figure(figsize=(12, 8))\nplt.semilogx(ns, confs*100)\nplt.ylabel(\"Confidence Level\")\nplt.xlabel('Sample size')\nplt.title(f\"interval={interval}\")\nplt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n\n\n\n\n\n\n\n\n\nWe need more than 10,000 samples to have confident estimate of \\(p\\).\nLet’s see how it changes with different interval values.\n\n\nCode\nintervals = np.arange(1, 5) / 100\nns = np.power(2, np.arange(6, 17)) # sample sizes\nconfs = np.array([[compute_confidence(n, interval) for n in ns] for interval in intervals]).squeeze()\n\nplt.figure(figsize=(12, 8))\nplt.semilogx(ns, confs.T*100, lw=3)\nplt.ylabel(\"Confidence Level\")\nplt.xlabel('Sample size')\nplt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\nplt.legend([f'{interval}' for interval in intervals]);\n\n\n\n\n\n\n\n\n\nAs interval gets tighter, we need more samples for a confidence level. Now, let’s write a function that calculates required sample size for given interval and confidence level.\n\n\nCode\nplt.figure(figsize=(12, 8))\nplt.semilogy(ns, (1 - confs.T)*100, lw=3)\nplt.ylabel(\"Error Level\")\nplt.xlabel('Sample size')\nplt.legend([f'{interval}' for interval in intervals]);\n\n\n\n\n\n\n\n\n\nThe error (\\(1 - \\text{confidence level}\\)) gets exponentially smaller as the sample size increases. Hence, we fit a 1D spline linear interpolator on logarithm of error with respect to sample size.\n\nfrom scipy.interpolate import interp1d\n\ndef find_sample_size(confidence, interval, p=1/2, eps=1e-32):\n    assert interval &gt;= .01\n    \n    ns = np.power(2, np.arange(1, 18))\n    confs = np.array([compute_confidence(n, interval) for n in ns]).squeeze()\n    confs = np.clip(confs, 0, 1)\n    inconfs = 1 - confs\n    f = interp1d(np.log(inconfs + eps), ns, kind='slinear')\n    return f(np.log(1-confidence))\n\nLet’s check for various confidence levels.\n\n\nMin sample size for conf=0.95, interval=0.04 is 659\n\n\n\n\nMin sample size for conf=0.95, interval=0.01 is 9943\n\n\n\n\nMin sample size for conf=0.999999, interval=0.01 is 60058"
  },
  {
    "objectID": "posts/confidence-intervals/index.html#conclusion",
    "href": "posts/confidence-intervals/index.html#conclusion",
    "title": "How many coin flips does high confidence need?",
    "section": "Conclusion",
    "text": "Conclusion\nLet’s say we tossed the coin for 659 times and the number of heads is 309. So, our estimate for \\(p\\) is 0.47. Does this mean \\(p\\) is likely to be in 0.47 ± 0.04? No, quite the opposite. The probability of heads is not probabilistic, it is a characteristic of the coin. We are trying to measure it. On the contrary, our estimate is probabilistic as we take finite number of samples to derive it. Hence, we can only talk about the likelihood of the estimate being close to true value. Specifically, the confidence interval states that there is 95% chance that the estimate is within ±4% interval of true value.\nThis method can be used to determine number of samples required for polls, if we assume that each individual’s answers are mutually independent. The good news is the sample size required does not depend on population size and confidence level gets exponentially better w.r.t. sample size."
  },
  {
    "objectID": "posts/confidence-intervals/index.html#references",
    "href": "posts/confidence-intervals/index.html#references",
    "title": "How many coin flips does high confidence need?",
    "section": "References",
    "text": "References\n\nEric Lehman, F. Thomson Leighton, Albert R. Meyer. Mathematics for Computer Science. Lecture Notes, available at https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-fall-2010/readings/MIT6_042JF10_notes.pdf, 2010."
  },
  {
    "objectID": "posts/py-cuda-kernel/index.html",
    "href": "posts/py-cuda-kernel/index.html",
    "title": "Benchmark CUDA kernel with Python",
    "section": "",
    "text": "In this tutorial, we’ll see the impact of block size on the performance of a CUDA kernel. We’ll use subprocess module in Python standard library to compile and execute a CUDA program which is defined in runtime.\nLet’s first verify that the device has a NVIDIA GPU and CUDA.\n\n!nvidia-smi\n\nWe’ll use a simple vector addition kernel which will be executed in parallel by many threads in multiple blocks on the GPU. The whole program is defined as a string. We’ll replace BLOCKS_SIZE variable in runtime to try different block size values.\n\nTEMPLATE = r\"\"\"\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;time.h&gt;\n#include &lt;cuda.h&gt;\n\n__global__ void vector_addition_kernel(float *a, float *b, float *out, int N)\n{\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i &lt; N)\n    {\n        out[i] = a[i] + b[i];\n    }\n}\n\nint main(void)\n{\n\n    const int N = 1 &lt;&lt; 28; // Number of elements in arrays\n    float *a, *b, *out;\n    float *a_d, *b_d, *out_d;\n\n    int BLOCK_SIZE = 1;\n    int NUM_BLOCKS = N / BLOCK_SIZE + (N % BLOCK_SIZE == 0 ? 0 : 1);\n\n    dim3 BLOCK_SIZE_DIM3 = dim3(BLOCK_SIZE, 1, 1);\n    dim3 NUM_BLOCKS_DIM3 = dim3(NUM_BLOCKS, 1, 1);\n\n    size_t size = N * sizeof(float);\n\n    // Allocate memory\n    a = (float *)malloc(size);\n    b = (float *)malloc(size);\n    out = (float *)malloc(size);\n\n    cudaMalloc(&a_d, size);\n    cudaMalloc(&b_d, size);\n    cudaMalloc(&out_d, size);\n\n    // Fill arrays with random values\n    for (int i = 0; i &lt; N; i++)\n    {\n        a[i] = rand() / (float)RAND_MAX;\n        b[i] = rand() / (float)RAND_MAX;\n    }\n\n    cudaMemcpy(a_d, a, size, cudaMemcpyHostToDevice);\n    cudaMemcpy(b_d, b, size, cudaMemcpyHostToDevice);\n\n    clock_t start = clock();\n\n    vector_addition_kernel&lt;&lt;&lt;NUM_BLOCKS_DIM3, BLOCK_SIZE_DIM3&gt;&gt;&gt;(a_d, b_d, out_d, N);\n    cudaDeviceSynchronize();\n\n    clock_t end = clock();\n    double elapsed_time_ms = 1000 * (double)(end - start) / CLOCKS_PER_SEC;\n    printf(\"Elapsed time: %f ms\\n\", N, elapsed_time_ms);\n\n    cudaMemcpy(out, out_d, size, cudaMemcpyDeviceToHost);\n\n    // cleanup the host memory\n    free(a);\n    free(b);\n    free(out);\n\n    cudaFree(a_d);\n    cudaFree(b_d);\n    cudaFree(out_d);\n}\n\"\"\"\n\n\nimport subprocess\nimport re\nimport matplotlib.pyplot as plt\n\ndef run_cuda_code(code: str):\n    # Save the generated CUDA code to a file\n    with open(\"program.cu\", \"w\") as f:\n        f.write(code)\n\n    # Compile the CUDA code\n    subprocess.run([\"nvcc\", \"-o\", \"program\", \"program.cu\"], check=True)\n\n    # Run the compiled executable and capture its output\n    result = subprocess.run([\"./program\"], capture_output=True, text=True)\n\n    # Extract the execution time from the output\n    execution_time = float(re.search(r\"Elapsed time: (\\d+\\.\\d+) ms\", result.stdout).group(1))\n    return execution_time\n\nblock_sizes = [2**i for i in range(11)]\nexecution_times = []\n\nfor block_size in block_sizes:\n    code = TEMPLATE.replace(\"int BLOCK_SIZE = 1;\", f\"int BLOCK_SIZE = {block_size};\")\n    execution_time = run_cuda_code(code)\n    execution_times.append(execution_time)\n\n\nimport math\n\nplt.figure(figsize=(10, 6))\nplt.plot(block_sizes, execution_times, marker='o', linestyle='-', color='b')\nplt.xscale(\"log\")\nplt.title('Program Execution Time by Block Size')\nplt.xlabel('Block Size')\nplt.ylabel('Execution Time (ms)')\nplt.xticks(block_sizes, labels=[f\"2^{int(math.log(block_size, 2))}\" for block_size in block_sizes])\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThe figure above shows a decrease in execution time with bigger block size until it hits 64 threads. Beyond this point, there’s a noticeable plateau, signifying no further gains in speed."
  },
  {
    "objectID": "posts/is-everything-an-abstraction/index.html",
    "href": "posts/is-everything-an-abstraction/index.html",
    "title": "Is Everything an Abstraction?",
    "section": "",
    "text": "Imagine peering into the heart of reality, striving to grasp the universe’s most fundamental secrets. As physicists delve deeper—unveiling particles smaller than atoms and forces more elusive than gravity—a provocative thought emerges: “Everything is an abstraction.” This idea isn’t merely poetic. It challenges the way we conceive of the cosmos and our theories about it. Are electrons, quarks, and fields actual entities out there in the world, or are they sophisticated tools our minds develop to navigate an ultimately unknowable reality?"
  },
  {
    "objectID": "posts/is-everything-an-abstraction/index.html#the-abstraction-conundrum",
    "href": "posts/is-everything-an-abstraction/index.html#the-abstraction-conundrum",
    "title": "Is Everything an Abstraction?",
    "section": "The Abstraction Conundrum",
    "text": "The Abstraction Conundrum\nOur journey into the microscopic world reveals an ever-deepening hierarchy of particles. From Democritus’s early atomism to modern quantum field theory, science has consistently exposed smaller and more intricate components of matter. Yet, no matter how far we probe, our experience is mediated by abstractions. An atom, once considered indivisible, reveals itself as a complex interplay of electrons orbiting a nucleus of protons and neutrons. Zoom in further, and electrons turn out to be manifestations of quantum fields, with no definitive “thingness.” At the Planck scale, our current theories blur, hinting that perhaps the fundamental fabric of the universe lies beyond direct comprehension, accessible only through conceptual layers that we construct.\nThis perspective echoes the thoughts of philosophers like Kant, who argued that our understanding of the world is shaped by our cognitive structures, preventing us from ever grasping the “thing-in-itself” (Kant, 1781). Even so, we continue to glean regularities and construct theories—though these theories might reflect the structure of our own minds as much as any independent reality."
  },
  {
    "objectID": "posts/is-everything-an-abstraction/index.html#instrumentalism-models-as-maps",
    "href": "posts/is-everything-an-abstraction/index.html#instrumentalism-models-as-maps",
    "title": "Is Everything an Abstraction?",
    "section": "Instrumentalism: Models as Maps",
    "text": "Instrumentalism: Models as Maps\nOne compelling way to interpret our scientific theories is instrumentalism, championed by philosophers such as van Fraassen and Duhem. Instrumentalists see theories primarily as tools—models that predict and organize data (Van Fraassen, 1980; Duhem, 1906). In this view, concepts like electrons or the Higgs boson serve as labels for observed patterns, not necessarily literal building blocks of an external world.\nHistorically, Newton described gravity as a universal force acting at a distance, offering a powerful “map” that predicted planetary orbits with impressive accuracy. Then, Einstein recast gravity as a curvature of spacetime (Einstein, 1915), providing a new lens through which to see the same phenomena. For instrumentalists, such radical shifts suggest that scientific progress isn’t about unearthing ultimate truths but about refining our models so they “work” better for prediction and coherence.\nMoreover, Nietzsche questioned the notion of an absolute truth altogether, proposing in works like Twilight of the Idols (1889) that our “truths” reflect human perspectives and power dynamics. Though not formally an instrumentalist, his skepticism resonates with the anti-realist claim that we build frameworks to serve our purposes, rather than to capture an elusive, objective reality."
  },
  {
    "objectID": "posts/is-everything-an-abstraction/index.html#realism-peering-beneath-the-veil",
    "href": "posts/is-everything-an-abstraction/index.html#realism-peering-beneath-the-veil",
    "title": "Is Everything an Abstraction?",
    "section": "Realism: Peering Beneath the Veil",
    "text": "Realism: Peering Beneath the Veil\nIn contrast, realists (notably Putnam, and in a nuanced way Popper with his critical rationalism) maintain that scientific theories aim to describe a mind-independent reality (Putnam, 1981; Popper, 1959). The remarkable success of scientific predictions—from quantum electrodynamics to astrophysical measurements—suggests, they argue, that we are latching onto genuine features of the universe.\nConsider how electrons show up consistently in chemistry labs, particle accelerators, and electronic devices. Realists believe that such consistency implies electrons are more than conceptual placeholders; they are part of the deep structure of nature. The so-called “no miracles” argument (elaborated by philosophers like Putnam) underscores this stance: if scientific theories did not tap into something real, it would be an extraordinary coincidence that they yield such precise and wide-ranging predictive power."
  },
  {
    "objectID": "posts/is-everything-an-abstraction/index.html#challenges-pessimistic-meta-induction-and-paradigm-shifts",
    "href": "posts/is-everything-an-abstraction/index.html#challenges-pessimistic-meta-induction-and-paradigm-shifts",
    "title": "Is Everything an Abstraction?",
    "section": "Challenges: Pessimistic Meta-Induction and Paradigm Shifts",
    "text": "Challenges: Pessimistic Meta-Induction and Paradigm Shifts\nYet realism faces formidable challenges. One is the pessimistic meta-induction, articulated by Laudan (Laudan, 1981). Since many once-successful theories—like the phlogiston theory of combustion or the luminiferous ether—were eventually discarded, one might infer that our current theories could face a similar fate. On a related note, Kuhn (Kuhn, 1962) argued that science undergoes paradigm shifts, where a prevailing framework is entirely superseded by another, suggesting that theoretical progress isn’t always a straightforward accumulation of truths.\nRealists commonly respond with structural realism, proposed by Worrall (Worrall, 1989). While the surface concepts of theories may change drastically—gravity as “force” vs. “curvature,” for instance—underlying mathematical structures often remain intact. Maxwell’s equations, for example, fit into the broader tapestry of quantum electrodynamics (QED). This continuity, structural realists contend, implies that even if our conceptual pictures shift, we preserve something that corresponds to real, stable features of the world."
  },
  {
    "objectID": "posts/is-everything-an-abstraction/index.html#the-boundaries-of-computation-turings-uncomputable-problems",
    "href": "posts/is-everything-an-abstraction/index.html#the-boundaries-of-computation-turings-uncomputable-problems",
    "title": "Is Everything an Abstraction?",
    "section": "The Boundaries of Computation: Turing’s Uncomputable Problems",
    "text": "The Boundaries of Computation: Turing’s Uncomputable Problems\nExpanding on these challenges to any notion of a “final” or “complete” theory, Alan Turing demonstrated that certain problems are fundamentally uncomputable. In his 1936 paper, On Computable Numbers, with an Application to the Entscheidungsproblem, Turing proved that no algorithmic method could decide the Halting Problem for all possible programs (Turing, 1936). This result shows there are intrinsic limits to what can be solved or described by computational means.\nSuch insights parallel our discussion on realism and instrumentalism:\n\nLimitations of Abstraction: From an instrumentalist angle, Turing’s uncomputable problems reflect the idea that our models—like algorithms—may be extraordinarily useful but inherently bounded. No matter how powerful our theories become, some phenomena or questions remain beyond complete formalization or prediction.\nDepth of Reality: For realists, Turing’s result underscores the complexity of the universe. The fact that certain aspects remain uncomputable does not imply that reality is purely mind-dependent. Instead, it highlights how much of reality lies outside our current (and perhaps any) theoretical framework, reinforcing the view that an objective world exists, but may exceed our conceptual or algorithmic grasp.\n\nTuring’s work thus reveals a fundamental tension between our drive to produce comprehensive theories and the inherent constraints of our cognitive and computational capacities. Much like Kuhn’s paradigm shifts or Laudan’s pessimistic meta-induction, Turing’s uncomputable problems remind us that some truths—or aspects of reality—may remain permanently off-limits to our most sophisticated methods."
  },
  {
    "objectID": "posts/is-everything-an-abstraction/index.html#beyond-realism-and-instrumentalism",
    "href": "posts/is-everything-an-abstraction/index.html#beyond-realism-and-instrumentalism",
    "title": "Is Everything an Abstraction?",
    "section": "Beyond Realism and Instrumentalism",
    "text": "Beyond Realism and Instrumentalism\nWhile realism and instrumentalism are pivotal stances in the philosophy of science, additional angles deepen and complicate the discussion:\n\nMathematical Underdetermination:\nWhy is mathematics—seemingly a human invention—so adept at describing nature? Physicist Eugene Wigner famously called this the “unreasonable effectiveness of mathematics.” Is this a testament to an underlying mathematical reality (realism), or does it simply show we keep creating mathematical frameworks that match our observations (instrumentalism)?\nCultural and Linguistic Frameworks:\nThinkers like Wittgenstein highlight how language and cultural contexts shape our categories and interpretations, suggesting science is a product of our cognitive-linguistic environment rather than a pure mirror of nature. This viewpoint can reinforce the instrumentalist claim that theories are more about use than truth.\nQuantum Observer Effect:\nQuantum interpretations like the Copenhagen Interpretation emphasize measurement’s role in bringing phenomena “into being,” suggesting a more instrumentalist reading. Meanwhile, Many-Worlds or Objective Collapse theories lean toward a realist perspective where the wavefunction (or some underlying state) exists independently of observation.\nMonism and Consciousness:\nSpinoza’s monism (Spinoza, 1677) posited that all of reality is one substance, merely perceived in various modes. Contemporary explorations of consciousness raise similar questions about the unity of mind and world, sometimes hinting that both realism and instrumentalism capture partial truths about a single, indivisible reality.\n\nIn each of these frontiers, we find fresh ways to push—and problematize—the boundaries of both realism and instrumentalism."
  },
  {
    "objectID": "posts/is-everything-an-abstraction/index.html#convergent-realism-and-epistemic-humility",
    "href": "posts/is-everything-an-abstraction/index.html#convergent-realism-and-epistemic-humility",
    "title": "Is Everything an Abstraction?",
    "section": "Convergent Realism and Epistemic Humility",
    "text": "Convergent Realism and Epistemic Humility\nDespite their differences, many philosophers share a humility about how definitive our theories can be. Convergent realism, advanced by thinkers like Psillos (Psillos, 1999), contends that even if earlier theories were discarded, core elements survived and evolved, guiding us ever closer to a comprehensive account of reality. Instrumentalists caution, however, that no matter how refined our models, they remain tools—valuable, yes, but not necessarily revealing a final “truth.”\nCombining these insights, one might hold that our theories do, in some sense, track real patterns in the world, yet remain incomplete artifacts shaped by human reasoning, culture, and computational limits. Even Turing’s findings about uncomputable problems can be viewed through both lenses—bolstering the notion that some structures of reality elude total capture, while simultaneously affirming there is something robust and consistent “out there” that spawns these unsolvable challenges."
  },
  {
    "objectID": "posts/is-everything-an-abstraction/index.html#final-reflections-is-everything-an-abstraction",
    "href": "posts/is-everything-an-abstraction/index.html#final-reflections-is-everything-an-abstraction",
    "title": "Is Everything an Abstraction?",
    "section": "Final Reflections: Is Everything an Abstraction?",
    "text": "Final Reflections: Is Everything an Abstraction?\nSo, is everything an abstraction? From an instrumentalist vantage, much of what we call “reality” is indeed a matrix of models and constructs devised to help us predict and navigate phenomena. From a realist perspective, those abstractions, while imperfect, are glimpses into a deeper, mind-independent world—glimpses refined by evidence and evolving theoretical frameworks.\nIn either case, the dialogue between realism and instrumentalism underscores the provisional nature of our knowledge. Paradigm shifts, uncomputable problems, and cultural lenses all act as reminders that our intellectual reach has limits, even as we continue pressing forward. Whether those limits indicate a fundamental incompleteness of our concepts or merely the grandeur of a reality we’ve yet to fully chart is a question that remains open, fueling both philosophical inquiry and scientific exploration.\nYet if what we call “reality” is fundamentally molded by our concepts and categories, then the theories, models, and laws we devise reveal more about how we interpret phenomena than about any mind-independent world. Rather than breeding mere caution, this stance provokes a deeper question: Is there a reality untouched by our abstractions, or do we forever inhabit the structures our minds create? On this view, the pursuit of knowledge becomes a matter of refining and revising these abstractions—seeing how far they can carry us, even as we accept they are never purely objective. For those who hold that the cosmos is ultimately a tapestry woven from our own frameworks, this is not a timid possibility but a bold rethinking of what it means to know anything at all."
  },
  {
    "objectID": "posts/is-everything-an-abstraction/index.html#references",
    "href": "posts/is-everything-an-abstraction/index.html#references",
    "title": "Is Everything an Abstraction?",
    "section": "References",
    "text": "References\n\nImmanuel Kant, Critique of Pure Reason (1781) – Distinguishes between phenomena and noumena, key to debates on the limits of cognition.\n\nBas van Fraassen, The Scientific Image (1980) – Foundational text for constructive empiricism, a form of instrumentalism.\n\nPierre Duhem, The Aim and Structure of Physical Theory (1906) – Early proponent of theories as frameworks for organizing observations.\n\nAlbert Einstein, “General Theory of Relativity” (1915) – Revolutionized the understanding of gravity as curvature of spacetime.\n\nThomas Kuhn, The Structure of Scientific Revolutions (1962) – Introduces the concept of paradigm shifts.\n\nHilary Putnam, Reason, Truth, and History (1981); Mathematics, Matter and Method (1963) – Explores scientific realism, including the “no miracles” argument.\n\nKarl Popper, The Logic of Scientific Discovery (1959) – Emphasizes falsifiability and critical rationalism.\n\nFriedrich Nietzsche, Twilight of the Idols (1889) – Critiques objective truth, highlighting the influence of perspective.\n\nLarry Laudan, “A Confutation of Convergent Realism” (1981) – Presents the pessimistic meta-induction.\n\nJohn Worrall, “Scientific Realism and the Plasticity of the Conceptual Structure of Theories” (1989) – Proposes structural realism.\n\nStathis Psillos, Scientific Realism: How Science Tracks Truth (1999) – Argues for convergent realism.\n\nBaruch Spinoza, Ethics (1677) – Advocates a monistic view of reality as a single substance.\n\nAlan Turing, On Computable Numbers, with an Application to the Entscheidungsproblem (1936) – Demonstrates the existence of uncomputable problems.\n\nAlan Turing, Computing Machinery and Intelligence (1950) – Discusses the limits of computation and machine “thought.”"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a software engineer with a passion for artificial intelligence. I built AI-centric products in various industries, including auto insurance, and healthcare. If you are looking for a strong hands-on engineer to innovate your product with AI, reach me out on LinkedIn.\nSkills: System design, cloud computing, machine learning, deep learning, natural language processing, retrieval-augmented generation, computer vision, data engineering, fine-tuning/serving large language models, CI/CD, Python, TypeScript…\nBeneath the code and cosmic splendor,\nA software engineer, a dreamer, a ponderer,\nReaching for AI, strong as thunder,\nIn this vast universe, a knowledge wanderer.\nSeeking super-human minds, intelligence to create,\nDenizens of cosmos, our limits we break,\nEntering the unknown, our legacy we make,\nNavigating the stars, our destiny we take.\nIn pursuit of immortality, as fates intertwine,\nZealous advances, medical marvels that shine,\nSolving mysteries with math, a race through time,\nAdept in physics, unraveling the cosmic design.\nGraceful on the tennis court, I embrace,\nLatin dances fill my heart, a rhythmic trace,\nA whirlwind of movement, through time and space,\nMoments in nature, where sunlight I chase.\nLending my thoughts to a digital space,\nOn the blog, I share wisdom with grace,\nVenturing into the realms of machine learning,\nEmbracing the secrets of AI, passion burning.\nScribing tales of software engineering,\nA worldwide community, knowledge we're bringing,\nIn the pursuit of the stars, our legacy endures."
  },
  {
    "objectID": "posts/fast-fourier-transform/index.html",
    "href": "posts/fast-fourier-transform/index.html",
    "title": "Fourier Transform",
    "section": "",
    "text": "Fourier transform decomposes a signal into the sinusoidal components with different amplitude, frequency and phases such that the sum of those components equals to the signal. It transforms a signal from time domain to frequency domain. Fourier transform is applicable to both continuous and discrete signals. In this post, we will only cover the discrete case.\nFor a signal with N points, Discrete Fourier Transform uses the following bases:\n\\(e^{i~2\\pi~k} = \\cos (2\\pi~k)+i\\sin (2\\pi~k)\\)\nfor k = -N/2 … N/2 where k=1 means 1 cycle per signal."
  },
  {
    "objectID": "posts/fast-fourier-transform/index.html#nyquists-sampling-theorem",
    "href": "posts/fast-fourier-transform/index.html#nyquists-sampling-theorem",
    "title": "Fourier Transform",
    "section": "Nyquist’s Sampling Theorem",
    "text": "Nyquist’s Sampling Theorem\nTo be able to measure a sinusoidal wave, we need to sample at least two points within its one full cycle, i.e., one point in half cycle. Therefore, we cannot measure the components that make more than N/2 cycles within N timesteps. This is known as Nyquist’s sampling theorem. In practice, because of the risk of sampling those two points near zero crossings, we are only confident about the components with frequencies lower than N/4.\nAs an example, let’s see the measurements from 1 Hz sine wave with various sampling rates. Nyquist’s sampling theorem requires at least 2 Hz sampling rate for this signal. As you can see in the figure below, the sampling rate of 1 Hz (blue) measures a flat line. While sampling rate of 2 Hz (orange) is theoretically sufficent, its amplitude is far from the original’s. As the sampling rate increases, the measurement becomes more accurate.\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nimport numpy as np\n\n\n# Original signal\nw = 1 # frequency of sine wave, [Hz]\nT = 3 # duration of signal [second]\nfs = 128 # underlying sampling rate of signal[Hz]\n\nt = np.linspace(0, T, fs * T)\ny = np.sin(2*np.pi*w*t)\nindices = np.arange(len(t))\nfs_list = [1, 2, 4, 10]\n\nsamples = []\nfor f in fs_list:\n    offset = np.random.randint(0, 10)\n    idx = indices[:-offset:fs//f] + offset\n    t_sampled = t[idx]\n    y_sampled = y[idx]\n    samples.append((f, t_sampled, y_sampled))\n\n\n\nCode\nplt.figure(figsize=(12, 8))\nplt.plot(t, y, color='magenta', lw=4);\n\nlegends = ['Original signal']\nfor f, ts, ys in samples:\n    plt.plot(ts, ys, marker='o', ms=10, linestyle='--', lw=4);\n    legends.append(f'Fsamp = {f}ω')\nplt.legend(legends, loc='lower left')\nplt.xlim([0, 2]);"
  },
  {
    "objectID": "posts/fast-fourier-transform/index.html#discrete-fourier-transform",
    "href": "posts/fast-fourier-transform/index.html#discrete-fourier-transform",
    "title": "Fourier Transform",
    "section": "Discrete Fourier Transform",
    "text": "Discrete Fourier Transform\nDiscrete Fourier Transform\n\\[\nX_k = \\sum_{n=0}^{N-1} x_n \\cdot e^{-i~2\\pi~k~n~/~N}\n\\]\nInverse Discrete Fourier Transform\n\\[\nx_n = \\frac{1}{N}\\sum_{k=0}^{N-1} X_k e^{i~2\\pi~k~n~/~N}\n\\]\nThe intuition behind first formula is that kth cosine component takes k/N cycles in 1 timestep or point interval. In other words, its angular velocity is 2πk/N radians/timestep. To make kth component’s peak align with nth point, we need adjust the phase of kth component. Since it takes 2πk/N radians in one timestep, until nth point, it takes 2πkn/N radians. Therefore, we need to delay this component for 2πkn/N radians. To delay, we need to subtract from its phase, which means rotating its complex representation in negative/counter-clock-wise direction. Hence, there is a minus sign in front.\nFrom the above formula, for N point signal, a naive Fourier transform algorithm has O(N2) time complexity. However, Cooley and Tukey (1965) proposed an O(N logN) algorithm; hence, named as Fast Fourier Transform. It benefits the symmetry in the problem and uses recursive divide-conquer approach. Checkout this detailed explanation of FFT algorithm, if you’re interested.\nNumpy provides FFT algorithm in numpy.fft subpackage along with some utilities.\nLet’s have a signal consisting of two sinusoidal waves with 5 Hz and 10 Hz and uniform noise. Since FFT makes recursive calls to divide the signal into two halves, the number of points in the signal must be power of 2.\n\nfs = 128 # sampling rate, [Hz]\nT = 1 # duration of signal, [second]\nt = np.linspace(0, T, fs)\nN = len(t)\n\n\ncomponents = [\n    (1, 5, np.pi/4), # amplitude, frequency [Hz], phase [radian]\n    (2, 10, 0),\n]\n\nsignal = sum([a * np.cos(2*np.pi*w*t + p) for (a, w, p) in components])\nnoise = 1*np.random.uniform(-1, 1, N)\nx = signal + noise\n\n\n\nCode\nplt.figure(figsize=(12, 6))\nplt.plot(t, x)\nplt.xlabel(\"Time [second]\");\n\n\n\n\n\n\n\n\n\nDFT produces N complex coefficients xk per each component with frequency of k. Here, k denotes normalized frequency with unit of cycle/point and it ranges from -0.5 to 0.5. We can convert these frequencies to Hertz, by substituting point with time interval between consecutive points.\n\\[\n\\frac{cycle}{point} = \\frac{cycle}{\\frac{t~seconds}{N~points}} = \\frac{N}{t} * \\frac{cycle}{second} = \\frac{N}{t}~Hz\n\\]\nNumpy provides a utility function numpy.fft.fftfreq that takes number of points in DFT and timestep which is multiplicative inverse of sampling rate.\n\nN = len(signal)\nxk = np.fft.fft(x)\nfreq = np.fft.fftfreq(n=N, d=1/fs)\n\n\nprint(xk[:5])\n\n[12.07258851 +0.j          3.04180456+13.42630117j\n  2.4691799  -4.18227651j  1.64226413 -0.16802957j\n  5.91780044 +1.05461309j]\n\n\n\nprint(freq)\n\n[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.\n  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.\n  28.  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.\n  42.  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.\n  56.  57.  58.  59.  60.  61.  62.  63. -64. -63. -62. -61. -60. -59.\n -58. -57. -56. -55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45.\n -44. -43. -42. -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31.\n -30. -29. -28. -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17.\n -16. -15. -14. -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.\n  -2.  -1.]\n\n\n\n# the frequencies\nnormalized_freq = np.fft.fftfreq(n=N)\nfreq_manual = normalized_freq * N / T\n\nassert np.allclose(freq, freq_manual)\n\n\n# Bring negative frequencies to the front of the array\nfr = np.roll(freq, N//2)\namp = np.roll(np.abs(xk), N//2)\nphase = np.roll(np.angle(xk), N//2)\n\n\n\nCode\nplt.figure(figsize=(12, 4))\nplt.bar(fr, amp)\nplt.ylabel(\"Amplitude\");\nplt.xlabel(\"Frequency [Hz]\")\n\n\nText(0.5, 0, 'Frequency [Hz]')"
  },
  {
    "objectID": "posts/fast-fourier-transform/index.html#inverse-discrete-fourier-transform",
    "href": "posts/fast-fourier-transform/index.html#inverse-discrete-fourier-transform",
    "title": "Fourier Transform",
    "section": "Inverse Discrete Fourier Transform",
    "text": "Inverse Discrete Fourier Transform\n\\[\nx_n = \\frac{1}{N}\\sum_{k=0}^{N-1} X_k e^{i~2\\pi~k~n~/~N}\n\\]\nNow, we have the information about each of the components in the signal. We can reconstruct the original signal, by combining those components. For this, we first convert each coefficient xk into cosine and sine waves by multiplying with \\(e^{i~2\\pi~k~n~/~N}\\). Then, we sum all these waves per component and normalize it by number of points.\nNumpy provides inverse FFT function numpy.fft.ifft, which takes N complex coefficients xk and outputs N complex numbers with very small imaginary parts. Remember,\n\\[e^{ix} = \\cos (x)+i\\sin (x)\\] \\[ cos(-x) = cos(x) \\] \\[ sin(-x) = -sin(x) \\]\nTherefore, while summing up the components, the imaginary parts (sine) for negative and positive frequencies cancels each other. Whereas, real parts (cosine) add up.\n\nrec_signal = np.real(np.fft.ifft(xk))\n\n\n\nCode\nplt.figure(figsize=(12, 6))\nplt.plot(signal, lw=2)\nplt.plot(rec_signal, lw=2)\nplt.xlabel(\"Time [second]\")\nplt.legend([\"Original\", \"Reconstructed\"]);"
  },
  {
    "objectID": "posts/fast-fourier-transform/index.html#components",
    "href": "posts/fast-fourier-transform/index.html#components",
    "title": "Fourier Transform",
    "section": "Components",
    "text": "Components\nLet’s see how FFT predicts the amplitude, frequency and phase parameters of the components.\n\n\nCode\ndef get_component_params(xk, freq):\n    component = 2 * xk[freq] # multiply by two to include -freq as well\n    phase = np.angle(component)\n    amplitude = np.abs(component) / N\n    return amplitude, phase\n\n\n\n\nCode\ndef make_component_signal(xk, freq, t):\n    amplitude, phase = get_component_params(xk, freq)\n    component_signal = amplitude * np.cos(2 * np.pi * freq * t + phase)\n    return component_signal\n\n\n\n\nCode\nfor a, f, ph in components:\n    amp, phase = get_component_params(xk, f)\n    print()\n    print(f\"Component {f} Hz\")\n    print('_'*40)\n    print()\n    print(f\"True  amplitude: {a:4.2f} phase: {np.rad2deg(ph):4.1f} deg\")\n    print(f\"Calc. amplitude: {amp:4.2f} phase: {np.rad2deg(phase):4.1f} deg\")\n    print('='*40)\n\n\n\nComponent 5 Hz\n________________________________________\n\nTrue  amplitude: 1.00 phase: 45.0 deg\nCalc. amplitude: 1.07 phase: 52.1 deg\n========================================\n\nComponent 10 Hz\n________________________________________\n\nTrue  amplitude: 2.00 phase:  0.0 deg\nCalc. amplitude: 1.97 phase: 17.2 deg\n========================================\n\n\nAs we can see, it is fairly accurate considering the noise added on the signal.\n\n\nCode\nc5 = make_component_signal(xk, 5, t)\nc10 = make_component_signal(xk, 10, t)\n\nplt.figure(figsize=(12, 6))\nplt.plot(t, signal, linewidth=2)\nplt.plot(t, c5, t, c10)\nplt.xlabel(\"Time [second]\")\nplt.legend(['Original signal', '5 Hz component', '10 Hz component']);"
  },
  {
    "objectID": "posts/fast-fourier-transform/index.html#conclusion",
    "href": "posts/fast-fourier-transform/index.html#conclusion",
    "title": "Fourier Transform",
    "section": "Conclusion",
    "text": "Conclusion\nFFT transforms a signal from time domain to frequency domain and for some problems, frequency domain is more feasible to work with. FFT has usages in many fields such as * Solving differential equations * Signal filtering algorithms * System identification"
  },
  {
    "objectID": "posts/fastai-custom-model/index.html",
    "href": "posts/fastai-custom-model/index.html",
    "title": "How to use a custom model with fastai cnn_learner?",
    "section": "",
    "text": "Code\ndef pprint_model(model, truncate=64):\n    print(\"=\"*80)\n    print(\"Model modules:\")\n    print(\"=\"*80)\n    print()\n    for i, (name, module) in enumerate(model.named_children()):\n        desc = repr(module).replace('\\n', '').replace('  ', ' ')[:64] + '...'\n        print(f\"{i+1} - {desc}\\n\")\nfastai library offers many pre-trained models for vision tasks. However, we sometimes need to use a custom model available in another library or created from scratch. In this post, we’ll see how to use fastai’s cnn_learner with a custom model.\ncnn_learner is a utility function which creates a Learner from given a pretrained CNN architecture such as resnet18.\nTo do that, it uses the model metadata from model_meta registry. model_meta registry is simply a mapping (dictionary) from architecture to its metadata.\nThe cut value is used for stripping off the existing classification head of the network so that we can add a custom head and fine-tune it for our task.\nThe split function is used when discriminative learning rate schema is applied such that the layers of a model are trained with different learning rates.\nThe stats refer to the channel means and standard deviations of the images in ImageNet dataset, which the model is pretrained on.\nThere are two alternative ways to to use a custom model not present in model registry: 1. Create a new helper function similar to cnn_learner that splits the network into backbone and head. Check out Zachary Mueller’s awesome blog post to see how it’s done. 2. Register the architecture in model_meta and use cnn_learner.\nWe will cover the second option in this post.\nLet’s first inspect an architecture registered already, e.g. resnet18.\nHere is its model meta data from the registry:\nfrom fastai.vision.all import *\nmodel_meta[resnet18]\n\n{'cut': -2,\n 'split': &lt;function fastai.vision.learner._resnet_split&gt;,\n 'stats': ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])}\nAnd the model layers:\nm = resnet18()\npprint_model(m)\n\n================================================================================\nModel modules:\n================================================================================\n\n1 - Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3),...\n\n2 - BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_runn...\n\n3 - ReLU(inplace=True)...\n\n4 - MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_m...\n\n5 - Sequential( (0): BasicBlock(  (conv1): Conv2d(64, 64, kernel_siz...\n\n6 - Sequential( (0): BasicBlock(  (conv1): Conv2d(64, 128, kernel_si...\n\n7 - Sequential( (0): BasicBlock(  (conv1): Conv2d(128, 256, kernel_s...\n\n8 - Sequential( (0): BasicBlock(  (conv1): Conv2d(256, 512, kernel_s...\n\n9 - AdaptiveAvgPool2d(output_size=(1, 1))...\n\n10 - Linear(in_features=512, out_features=1000, bias=True)...\ncreate_body function called by create_cnn_model which is called in cnn_learner, strips off the head by cut index as such:\nIn our case, it’ll remove the last two layers of resnet18 network: AdaptiveAvgPool2d and fully connected Linear layer.\nbody = create_body(resnet18, pretrained=False, cut=model_meta[resnet18]['cut'])\npprint_model(body)\n\n================================================================================\nModel modules:\n================================================================================\n\n1 - Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3),...\n\n2 - BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_runn...\n\n3 - ReLU(inplace=True)...\n\n4 - MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_m...\n\n5 - Sequential( (0): BasicBlock(  (conv1): Conv2d(64, 64, kernel_siz...\n\n6 - Sequential( (0): BasicBlock(  (conv1): Conv2d(64, 128, kernel_si...\n\n7 - Sequential( (0): BasicBlock(  (conv1): Conv2d(128, 256, kernel_s...\n\n8 - Sequential( (0): BasicBlock(  (conv1): Conv2d(256, 512, kernel_s...\nSimilarly, we need to determine the cut index for the custom model we use. Let’s try EfficientNetB0 architecture available in torchvision library. First, we inspect the network layers to find out where to split it into backbone and head.\nfrom torchvision.models import efficientnet_b0\n\nm = efficientnet_b0()\npprint_model(m)\n\n================================================================================\nModel modules:\n================================================================================\n\n1 - Sequential( (0): ConvNormActivation(  (0): Conv2d(3, 32, kernel_...\n\n2 - AdaptiveAvgPool2d(output_size=1)...\n\n3 - Sequential( (0): Dropout(p=0.2, inplace=True) (1): Linear(in_fea...\nAs it can be seen, the pooling layer is at index -2, which corresponds to the cut value. We’ll use the default_split for split function and ImageNet stats as the model is pre-trained on it.\nfrom fastai.vision.learner import default_split\nmodel_meta[efficientnet_b0] = {'cut': -2, 'split': default_split, 'stats': imagenet_stats}"
  },
  {
    "objectID": "posts/fastai-custom-model/index.html#train-and-test-the-model",
    "href": "posts/fastai-custom-model/index.html#train-and-test-the-model",
    "title": "How to use a custom model with fastai cnn_learner?",
    "section": "Train and test the model",
    "text": "Train and test the model\nNow we can create a cnn_learner since our custom architecture is registered. Let’s create a toy dataloaders to train and test our model.\n\ndef label_func(f): \n    return f[0].isupper()\n\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path / \"images\")\npattern = r'^(.*)_\\d+.jpg'\ndls = ImageDataLoaders.from_name_re(path, files, pattern, item_tfms=Resize(224))\n\ndls.show_batch()\n\n\n    \n        \n      \n      100.00% [811712512/811706944 00:20&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n\n\n\nlearn = cnn_learner(\n    dls, \n    arch=efficientnet_b0,\n    pretrained=True,\n    metrics=accuracy, \n).to_fp16()\n\nDownloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-3dd342df.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-3dd342df.pth\n\n\n\n\n\nLet’s verify that the body and head are created correctly.\n\npprint_model(learn.model)\n\n================================================================================\nModel modules:\n================================================================================\n\n1 - Sequential( (0): Sequential(  (0): ConvNormActivation(   (0): Co...\n\n2 - Sequential( (0): AdaptiveConcatPool2d(  (ap): AdaptiveAvgPool2d(...\n\n\n\nLet’s inspect the custom head of the model:\n\npprint_model(learn.model[-1])\n\n================================================================================\nModel modules:\n================================================================================\n\n1 - AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp...\n\n2 - Flatten(full=False)...\n\n3 - BatchNorm1d(2560, eps=1e-05, momentum=0.1, affine=True, track_ru...\n\n4 - Dropout(p=0.25, inplace=False)...\n\n5 - Linear(in_features=2560, out_features=512, bias=False)...\n\n6 - ReLU(inplace=True)...\n\n7 - BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_run...\n\n8 - Dropout(p=0.5, inplace=False)...\n\n9 - Linear(in_features=512, out_features=37, bias=False)...\n\n\n\nAs it’s seen, cnn_learner created a new classification head starting with a pooling layer while keeping the original body from the pre-trained model.\nLet’s train and test our model for multi-label classification task.\n\nlearn.lr_find(start_lr=1e-05, end_lr=1e-1)\n\n\n\n\nSuggestedLRs(valley=0.0014454397605732083)\n\n\n\n\n\n\n\n\n\n\nlearn.fine_tune(3, base_lr=2e-3, freeze_epochs=3)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n2.515583\n0.687396\n0.801759\n01:12\n\n\n1\n1.083641\n0.469568\n0.852503\n01:08\n\n\n2\n0.687156\n0.430938\n0.866035\n01:07\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.414339\n0.401623\n0.876861\n01:27\n\n\n1\n0.351286\n0.369699\n0.872801\n01:27\n\n\n2\n0.272294\n0.349316\n0.881597\n01:27\n\n\n\n\n\n\nlearn.show_results()"
  },
  {
    "objectID": "posts/fastai-huggingface-datasets/index.html",
    "href": "posts/fastai-huggingface-datasets/index.html",
    "title": "How to use HuggingFace datasets with fastai?",
    "section": "",
    "text": "In this tutorial, we will show you how to use a HuggingFace dataset with fastai to train a model for image classification. We will use the Beans dataset, which consists of images of beans with three different types of diseases."
  },
  {
    "objectID": "posts/fastai-huggingface-datasets/index.html#step-1-install-the-required-libraries",
    "href": "posts/fastai-huggingface-datasets/index.html#step-1-install-the-required-libraries",
    "title": "How to use HuggingFace datasets with fastai?",
    "section": "Step 1: Install the required libraries",
    "text": "Step 1: Install the required libraries\nBefore starting, we need to install the required libraries. Run the following commands to install fastai and HuggingFace’s datasets:\n\n!pip install -Uqq fastai\n!pip install -Uqq datasets\n\nLogin to HuggingFace Hub to download the dataset.\n\n!huggingface-cli login"
  },
  {
    "objectID": "posts/fastai-huggingface-datasets/index.html#step-2-import-the-required-modules",
    "href": "posts/fastai-huggingface-datasets/index.html#step-2-import-the-required-modules",
    "title": "How to use HuggingFace datasets with fastai?",
    "section": "Step 2: Import the required modules",
    "text": "Step 2: Import the required modules\n\nimport torch\nfrom fastai.data.all import *\nfrom fastai.vision.all import *\nfrom datasets import load_dataset"
  },
  {
    "objectID": "posts/fastai-huggingface-datasets/index.html#step-3-load-the-dataset",
    "href": "posts/fastai-huggingface-datasets/index.html#step-3-load-the-dataset",
    "title": "How to use HuggingFace datasets with fastai?",
    "section": "Step 3: Load the dataset",
    "text": "Step 3: Load the dataset\nLet’s load Beans dataset, which is a dataset of images of beans taken in the field using smartphone cameras. It consists of 3 classes: 2 disease classes and the healthy class.\n\nraw_ds = load_dataset(\"beans\")\n\n\nraw_ds\n\nDatasetDict({\n    train: Dataset({\n        features: ['image_file_path', 'image', 'labels'],\n        num_rows: 1034\n    })\n    validation: Dataset({\n        features: ['image_file_path', 'image', 'labels'],\n        num_rows: 133\n    })\n    test: Dataset({\n        features: ['image_file_path', 'image', 'labels'],\n        num_rows: 128\n    })\n})\n\n\nThe dataset is splitted into train, validation, and test sets.\nLet’s see label names.\n\nclass_names = raw_ds['train'].features['labels'].names\nclass_names\n\n['angular_leaf_spot', 'bean_rust', 'healthy']"
  },
  {
    "objectID": "posts/fastai-huggingface-datasets/index.html#step-4-preprocess-the-dataset",
    "href": "posts/fastai-huggingface-datasets/index.html#step-4-preprocess-the-dataset",
    "title": "How to use HuggingFace datasets with fastai?",
    "section": "Step 4: Preprocess the dataset",
    "text": "Step 4: Preprocess the dataset\nOften, we need preprocessing raw data before training a model with it. HuggingFace datasets library provides two methods for preprocessing:\n\nmap: This method is used to apply a function to each example in the dataset, possibly in a batched manner. The function can be applied to one or more columns of the dataset, and the result can be stored in a new column or overwrite the existing one. The map function also allows you to remove some columns from the dataset, if needed. This method is useful for preprocessing the dataset, such as resizing images, tokenizing text, or encoding categorical features. It caches outputs so that they’re not computed again.\nset_transforms: This method is used to set a transform function that is applied on-the-fly when accessing examples from the dataset. This means that the dataset is not modified in-place, and the transform function is applied only when the examples are accessed. This method is useful for applying data augmentation techniques or normalization that should be applied dynamically during training without modifying the dataset beforehand.\n\nLet’s resize each image to 224x244.\n\ndef preprocess(records):\n    records[\"image\"] = [image.convert(\"RGB\").resize((224, 224)) for image in records[\"image\"]]\n    return records\n\nBefore batching samples, we need to remove unnecessary columns in the dataset such as image_file_path.\n\nds = raw_ds.map(preprocess, remove_columns=[\"image_file_path\"], batched=True)\n\nLoading cached processed dataset at /Users/bdsaglam/.cache/huggingface/datasets/beans/default/0.0.0/90c755fb6db1c0ccdad02e897a37969dbf070bed3755d4391e269ff70642d791/cache-d335def00fc26298.arrow\nLoading cached processed dataset at /Users/bdsaglam/.cache/huggingface/datasets/beans/default/0.0.0/90c755fb6db1c0ccdad02e897a37969dbf070bed3755d4391e269ff70642d791/cache-9b27f1e864a73628.arrow\nLoading cached processed dataset at /Users/bdsaglam/.cache/huggingface/datasets/beans/default/0.0.0/90c755fb6db1c0ccdad02e897a37969dbf070bed3755d4391e269ff70642d791/cache-417ffd63aef737e6.arrow\n\n\n\nds['train'][0]\n\n{'image': &lt;PIL.PngImagePlugin.PngImageFile image mode=RGB size=224x224&gt;,\n 'labels': 0}\n\n\nWe won’t use set_transform as fastai’s DataBlock can apply item-level and batch-level transforms, e.g. data augmentations, normalization. When we use a pretrained model, it already applies necessary transforms such as normalization."
  },
  {
    "objectID": "posts/fastai-huggingface-datasets/index.html#step-5-create-the-datablock",
    "href": "posts/fastai-huggingface-datasets/index.html#step-5-create-the-datablock",
    "title": "How to use HuggingFace datasets with fastai?",
    "section": "Step 5: Create the DataBlock",
    "text": "Step 5: Create the DataBlock\nNow, we can create dataloaders for the dataset using DataBlock from fastai. As the dataset is already splitted into train, validation, and test sets, we don’t need to split it further. Hence, we will use nosplit function.\n\ndef nosplit(items): \n    return list(range(len(items))), []\n\n\ndblock = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_x=lambda record: record['image'],\n    get_y=lambda record: class_names[record['labels']],\n    splitter = nosplit,\n)\n\ntrain_dl = dblock.dataloaders(ds['train']).train\nvalid_dl = dblock.dataloaders(ds['validation']).train\ndls = DataLoaders(train_dl, valid_dl)\n\ndls.show_batch()\n\n\n\n\n\n\n\n\n\nlen(dls.train.dataset),len(dls.valid.dataset)\n\n(1034, 133)"
  },
  {
    "objectID": "posts/fastai-huggingface-datasets/index.html#step-6-training",
    "href": "posts/fastai-huggingface-datasets/index.html#step-6-training",
    "title": "How to use HuggingFace datasets with fastai?",
    "section": "Step 6: Training",
    "text": "Step 6: Training\nLet’s fine-tune a pretrained ResNet model on our dataset using Learner.fine_tune method.\n\nlearn = vision_learner(\n    dls, \n    resnet34, \n    loss_func=CrossEntropyLossFlat(),\n    metrics=accuracy, \n)\n\n\n# Find a good learning rate\nlearn.lr_find()\n\nSuggestedLRs(valley=0.0004786300996784121)\n\n\n\n\n\n\n\n\n\n\n# Fine-tune the model\nlearn.fine_tune(1, 1e-3, freeze_epochs=2)"
  },
  {
    "objectID": "posts/fastai-huggingface-datasets/index.html#step-7-evaluation",
    "href": "posts/fastai-huggingface-datasets/index.html#step-7-evaluation",
    "title": "How to use HuggingFace datasets with fastai?",
    "section": "Step 7: Evaluation",
    "text": "Step 7: Evaluation\nOnly after 3 epochs, fine-tuned model achieves 90.6% accuracy on validation set, not too bad!\n\nloss, accuracy = learn.validate(dl=dls.valid)\nprint(f\"Loss {loss:.6f}\\nAccuracy: {accuracy:.2%}\")\n\nLoss 0.306838\nAccuracy: 90.62%\n\n\nLet’s check predictions visually.\n\nlearn.show_results(dl=valid_dl)\n\n\n\n\n\n\n\n\nLet’s predict and evaluate on test set.\n\ntst_dl = dls.test_dl(ds['test'], with_labels=True)\nprobs, targets, preds = learn.get_preds(dl=tst_dl, with_decoded=True)\n\n\nloss, accuracy = learn.validate(dl=tst_dl)\nprint(f\"Loss {loss:.6f}\\nAccuracy: {accuracy:.2%}\")\n\nLoss 0.288472\nAccuracy: 89.06%"
  },
  {
    "objectID": "posts/halting-problem/index.html",
    "href": "posts/halting-problem/index.html",
    "title": "Halting problem",
    "section": "",
    "text": "Halting problem was proved to be unsolvable by Alan Turing in his seminal paper, in 1936. It states that there cannot be a general algorithm to decide whether an arbitrary pair of computer program and input halts (finishes execution) or not.\nSurprisingly, the proof of such a bold statement is ingeniously simple.\nProof by contradiction.\nSuppose that there exists a computer program H that solves halting problem. For a pair of computer program P and input I, H outputs true if P finished with I. Otherwise, it outputs false. How H can do this is not important for the sake of proof. In Python, H would be roughly such:\ndef H(program, inpt):\n    # with some black magic\n    if program_halts_on_inpt:\n        return True\n    else:\n        return False\nNow, having H, let’s define another computer program G, which receives a computer program P. G copies P and asks H whether P halts on P. If H decides that P halts on P, G diabolically loops forever. If H decides otherwise, G halts.\ndef G(program):\n    if H(program, program):\n        while True:\n            pass\n    else:\n        return\nNow, the interesting part. Let’s run G with G as input, i.e. call G(G). It calls H(G, G) and there are two possible outcomes. 1. H decides that program G halts on input G and returns true. Then, inside G, first brach of if becomes active and G loops forever, i.e. does not halt. 1. H decides that program G does not halt on input G and returns false. Then, inside G, else branch becomes active and G halts.\nThis is a contradiction. Hence, H cannot exist."
  },
  {
    "objectID": "posts/initialization-in-nn/index.html",
    "href": "posts/initialization-in-nn/index.html",
    "title": "Why is initialization important in neural networks?",
    "section": "",
    "text": "Today, when we build a deep neural network with standard layers and activation functions, we don’t worry much about the initialization of network’s parameters. But, this wasn’t always so. Exploding/vanishing gradients and numerical instability issues were likely if you were not careful about initialization. We hardly encounter such problems nowadays, partly because ReLU has become de-facto non-linearity choice and modern neural network frameworks use the right initialization schemes under the hood. But, why is initialization important at all?\nWe can consider a neural network as a collection learnable parameters that are initialized randomly and updated with gradient descent. In theory, this must find the optimal parameters, no matter the initial values. However, since the floating point representations used in computers have a finite range and precision, picking wrong initial values may cause out-of-range numerical values and cripple training process.\nOK, how about constraining all the initial values to a specific range, such as [-1, 1] or [-0.1, 0.1]? Would it solve the problem? Unfortunately, it may not. In modern deep neural networks, the input goes through many layers, each changing incoming data in a specific way. Therefore, the changes accumulate through the network and eventually may cause the values to grow too large or too small. To solve this problem for all networks with an arbitrary number of layers, we use proven initialization schemes that keep the variance of data and gradients unchanged through the neural network as much as possible."
  },
  {
    "objectID": "posts/initialization-in-nn/index.html#how-does-the-properties-of-a-distribution-changes-through-various-transformations",
    "href": "posts/initialization-in-nn/index.html#how-does-the-properties-of-a-distribution-changes-through-various-transformations",
    "title": "Why is initialization important in neural networks?",
    "section": "How does the properties of a distribution changes through various transformations?",
    "text": "How does the properties of a distribution changes through various transformations?\n\n\nCode\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport seaborn\n\n# Set seaborn aesthetic parameters to defaults\nseaborn.set()\nseaborn.set_style('whitegrid')\nseaborn.set_palette(\"dark\")\n\nimport numpy as np\nimport torch\nnp.random.seed(42)\n\ndef plot_distributions(*arrays, bins=None, fig_width=12, fig_height=4, xlim=None):\n    n = len(arrays)\n    fig = plt.figure(figsize=(fig_width, fig_height*n))\n    for i, x in enumerate(arrays):\n        ax = fig.add_subplot(n, 1, i+1)\n        seaborn.distplot(x, bins=bins)\n        ax.set_title(stats_desc(x))\n        ax.set_xlim(xlim)\n\n\n\ndef plot_distribution(x, bins=None, fig_width=12, fig_height=4, xlim=None):\n    fig = plt.figure(figsize=(fig_width, fig_height))\n    seaborn.distplot(x, bins=bins)\n    plt.title(name + ' ' + stats_desc(x))\n    plt.xlim(xlim)\n\n\ndef stats_desc(x):\n    return f\"μ: {x.mean():+.3f}  σ: {x.std():.3f}\"\n\nLet’s assume that the input is from a uniform distribution between [-1, 1].\n\\(x \\sim U[-1, 1]\\)\n\nn = 1000\nx = np.random.uniform(-1, 1, n)\nplot_distributions(x, bins=100)\n\n\n\n\n\n\n\n\n\nSummation\nThe sum of two independent uniform distribution is a distribution with\n\\(\\quad \\mu= \\mu_1 + \\mu_2 \\quad \\sigma^2=\\sigma_1^2 + \\sigma_2^2\\)\n\ny = np.random.uniform(0, 10, n)\nz = x + y\n\nprint(f\"Added a uniform dist. {stats_desc(y)}\")\n\nplot_distributions(x, y, xlim=(-2, 15), fig_width=12)\n\nAdded a uniform dist. μ: +5.070  σ: 2.920\n\n\n\n\n\n\n\n\n\n\n\nScale and shift\n\ny = 2 * x + 1\n\nplot_distributions(x, y, xlim=(-4, 4))\n\n\n\n\n\n\n\n\n\n\nAffine transformation\nA linear layer (or fully connected layer) with weights, \\(W \\sim U[-c, c]\\) , approximately transforms a uniform distribution \\(x \\sim U[-1, 1]\\) to a normal distribution \\(y \\sim \\mathcal N(\\mu, \\sigma)\\)\n\\[ \\mu = 0 \\quad \\sigma^2 = n_{in} ~ \\sigma_{input}^2 ~ \\sigma_{w}^2 \\]\n\\(n_{in}\\): size of input, also denoted as, \\(\\text{fan}_{in}\\)\nLet’s initialize weights with \\(W \\sim U[-1, 1]\\).\n\nW = np.random.uniform(-1, 1, size=(n, n))\nbias = np.random.uniform(-1, 1, size=n)\n\nprint(f\"Weight {stats_desc(W)}\")\nprint(f\"Bias   {stats_desc(bias)}\")\n\ny = x @ W + bias\nplot_distributions(x, y, xlim=(-30, 30))\n\nWeight μ: +0.001  σ: 0.577\nBias   μ: +0.008  σ: 0.566\n\n\n\n\n\n\n\n\n\nThe variance increased dramatically even after one layer.\nLet’s try smaller weights with \\(W \\sim U[-0.1, 0.1]\\)\n\nW = np.random.uniform(-.1, .1, size=(n, n))\nbias = np.random.uniform(-.1, .1, size=n)\n\nprint(f\"Weight {stats_desc(W)}\")\nprint(f\"Bias   {stats_desc(bias)}\")\n\ny = x @ W + bias\nplot_distributions(x, y, xlim=(-4, 4))\n\nWeight μ: -0.000  σ: 0.058\nBias   μ: +0.002  σ: 0.057\n\n\n\n\n\n\n\n\n\nThis looks better, the variance goes up slightly. Let’s see how it performs with multiple layers with various sizes.\n\nclass Affine:\n    def __init__(self, dim_in, dim_out, weight_bound=0.1, bias_bound=0.1):\n        self.W = np.random.uniform(-weight_bound, weight_bound, size=(dim_in, dim_out))\n        self.b = np.random.uniform(-bias_bound, bias_bound, size=dim_out)\n\n    def __call__(self, x):\n        return x @ self.W + self.b\n\n\nlayers = [\n    Affine(x.shape[-1], 256),\n    Affine(256, 512),\n    Affine(512, 1024),\n    Affine(1024, 1024),\n    Affine(1024, 1024),\n    Affine(1024, 1024),\n]\n\n\nactivations = []\ny = x\nfor layer in layers:\n    y = layer(y)\n    activations.append(y)\n\n\nplot_distributions(x, *activations, xlim=(-25, 25))\n\n\n\n\n\n\n\n\nThe variance keeps increasing after each layer and at the end it is much larger than the beginning’s."
  },
  {
    "objectID": "posts/initialization-in-nn/index.html#solution",
    "href": "posts/initialization-in-nn/index.html#solution",
    "title": "Why is initialization important in neural networks?",
    "section": "Solution",
    "text": "Solution\nWe need to keep variance of data and gradients unchanged through the network as much as possible.\nIn forward pass,\n\\[ \\sigma^2 = n_{in} ~ \\sigma_{input}^2 ~ \\sigma_{w}^2 \\]\nTo keep variance unchanged, we want to have\n\\[ n_{in} ~ \\sigma_w^2 = 1 \\quad \\sigma_{w}^2 = \\frac{1}{n_{in}} \\]\nA similar tranformation happens in backward pass when gradients flow in reverse direction.\n\\[ \\sigma_\\text{grad before layer}^2 = n_{out}~\\sigma_{w}^2~\\sigma_{\\text{grad after layer}}^2 \\]\nAgain, we want to have\n\\[ n_{out} ~ \\sigma_w^2 = 1 \\quad \\sigma_{w}^2 = \\frac{1}{n_{out}} \\]\nUnless \\(n_{in}=n_{out}\\), we cannot satisfy both constraints. Therefore, we do our best.\n\\[\n\\frac{1}{2} (n_{in} + n_{out}) \\sigma_{w}^2 = 1 \\\\ \\sigma_w^2 = \\frac{2}{n_{in} + n_{out}}\n\\]\nRemember the variance of a uniform distribution with U[a, b]\n\\[ \\sigma^2 = \\frac{(b-a)^2}{12} \\]\n\\[ \\sigma_{w}^2 = \\frac{(2c)^2}{12} = \\frac{c^2}{3} \\]\nHence,\n\\[ \\frac{c^2}{3} = \\frac{2}{n_{in} + n_{out}} \\quad c = \\sqrt{\\frac{6}{n_{in} + n_{out}}} \\]\nThis is known as Xavier initialization.\nNow, let’s initialize the weights according to Xavier initialization, see how it affects variance.\n\nclass XavierAffine:\n    def __init__(self, dim_in, dim_out):\n        weight_bound = np.sqrt(6/(dim_in + dim_out))\n        bias_bound = np.sqrt(2/(dim_in + dim_out))\n        \n        self.W = np.random.uniform(-weight_bound, weight_bound, size=(dim_in, dim_out))\n        self.b = np.random.uniform(-bias_bound, bias_bound, size=dim_out)\n\n    def __call__(self, x):\n        return x @ self.W + self.b\n\n\nxav_layers = [\n    XavierAffine(x.shape[-1], 256),\n    XavierAffine(256, 512),\n    XavierAffine(512, 1024),\n    XavierAffine(1024, 1024),\n    XavierAffine(1024, 1024),\n    XavierAffine(1024, 1024),\n]\n\n\nactivations = []\ny = x\nfor layer in xav_layers:\n    y = layer(y)\n    activations.append(y)\n\n\nplot_distributions(x, *activations, xlim=(-2, 2))\n\n\n\n\n\n\n\n\nAs it’s seen, with Xaiver initialization, the variance does not vary much through the layers."
  },
  {
    "objectID": "posts/initialization-in-nn/index.html#references",
    "href": "posts/initialization-in-nn/index.html#references",
    "title": "Why is initialization important in neural networks?",
    "section": "References",
    "text": "References\n\nhttp://d2l.ai/chapter_multilayer-perceptrons/numerical-stability-and-init.html#xavier-initialization"
  },
  {
    "objectID": "posts/llm-providers-structured-output/index.html",
    "href": "posts/llm-providers-structured-output/index.html",
    "title": "Structured output quirks",
    "section": "",
    "text": "TLDR: Many LLM providers claim to be OpenAI API compatible but there are some differences in the response_format parameter for structured output usage.\n\nOpenAI: Supports both json_schema and json_object for response format type. When using json_object, you must include the JSON schema in the system prompt.\nGroq: Supports json_object and requires including the schema in the system prompt. Providing the schema via the value argument makes no difference.\nTGI: Succeeds only when using json_object with the schema provided as a value in the response_format; simply including the schema in the prompt isn’t sufficient.\n\n\nStructured outputs for large language models (LLMs) refer to the ability of these models to generate outputs in a predefined format or structure. This can include generating JSON objects, tables, or other structured data formats that are easier to parse and utilize in downstream applications.\nLet’s use a simple example to illustrate this. Suppose we have a text and we want to extract the entities and relations from it. We can use a structured output to extract the entities and relations from the text.\n\n\nCode\nimport json\nimport os\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\n\nload_dotenv()\n\ndef jprint(obj):\n    print(json.dumps(obj, indent=2))\n\n\n\nclass EntityRelationExtraction(BaseModel):\n    triples: list[str] = Field(description=\"The `subject | predicate | object` triples extracted from the text.\")\n\nschema = EntityRelationExtraction.model_json_schema()\njprint(schema)\n\n{\n  \"properties\": {\n    \"triples\": {\n      \"description\": \"The `subject | predicate | object` triples extracted from the text.\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"Triples\",\n      \"type\": \"array\"\n    }\n  },\n  \"required\": [\n    \"triples\"\n  ],\n  \"title\": \"EntityRelationExtraction\",\n  \"type\": \"object\"\n}\n\n\nWe’ll compare OpenAI, Groq, and Hugging Face Text Generation Inference (TGI) chat completion APIs in this notebook for the structured output usage.\n\nopenai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\ngroq_client = OpenAI(base_url=os.getenv(\"GROQ_BASE_URL\"), api_key=os.getenv(\"GROQ_API_KEY\"))\n\n# Assume Llama 3.1 8B model is served on localhost:8008 using TGI\ntgi_client = OpenAI(base_url=\"http://0.0.0.0:8008/v1\", api_key='_')\n\n\ntext = \"\"\"\nEinstein was born in Ulm, Germany. He discovered the special theory of relativity in 1905. He was awarded the Nobel Prize in Physics in 1921. His first wife, Mileva Marić, was a Serbian mathematician and physicist.\n\"\"\".strip()\n\n\n\nCode\nfrom typing import Any\n\ndef erx_json(\n    client: OpenAI,\n    text: str,\n    model: str,\n    temperature: float = 0.0,\n    response_format: dict | None = None,\n    include_schema_in_system_prompt: bool = False,\n) -&gt; tuple[Any, EntityRelationExtraction | None, Exception | None]:\n    system_prompt = \"You are a helpful assistant that extracts `subject | predicate | object` triples from text.\"\n    if include_schema_in_system_prompt:\n        system_prompt += f\"\\n# JSON Schema\\n{schema}\"\n    \n    response = None\n\n    try:\n        response = client.chat.completions.create(\n            model=model,\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": text},\n            ],\n            temperature=temperature,\n            response_format=response_format,\n        )\n    except Exception as e:\n        error = e\n        return None, None, error\n    \n    try:\n        parsed = EntityRelationExtraction.model_validate_json(\n            response.choices[0].message.content\n        )\n    except Exception as e:\n        error = e\n        return response, None, error\n\n    return response, parsed, None\n\n\n\n\nCode\ninputs = [\n    {\n        \"name\": \"json_schema\",\n        \"response_format\": {\n            \"type\": \"json_schema\",\n            \"json_schema\": {\"name\": \"EntityRelationExtraction\", \"schema\": schema}\n        },\n        \"include_schema_in_system_prompt\": False,\n    },\n    {\n        \"name\": \"json_schema_w_schema_in_system_prompt\",\n        \"response_format\": {\n            \"type\": \"json_schema\",\n            \"json_schema\": {\"name\": \"EntityRelationExtraction\", \"schema\": schema}\n        },\n        \"include_schema_in_system_prompt\": True,\n    },\n    {\n        \"name\": \"json_object\",\n        \"response_format\": {\n            \"type\": \"json_object\",\n            \"value\": None\n        },\n        \"include_schema_in_system_prompt\": False,\n    },\n    {\n        \"name\": \"json_object_w_schema_in_system_prompt\",\n        \"response_format\": {\n            \"type\": \"json_object\",\n            \"value\": None\n        },\n        \"include_schema_in_system_prompt\": True,\n    },\n    {\n        \"name\": \"json_object_value_schema\",\n        \"response_format\": {\n            \"type\": \"json_object\",\n            \"value\": schema\n        },\n        \"include_schema_in_system_prompt\": False,\n    },\n    {\n        \"name\": \"json_object_value_schema_w_system_prompt\",\n        \"response_format\": {\n            \"type\": \"json_object\",\n            \"value\": schema\n        },\n        \"include_schema_in_system_prompt\": True,\n    },\n]\n\n\n\n\nCode\nresults = []\n\nfor client, provider, model in [\n    (openai_client, \"OpenAI\", \"gpt-4o-mini\"),\n    (groq_client, \"Groq\", \"llama-3.1-8b-instant\"),\n    (tgi_client, \"TGI\", \"llama-3-8b\"),\n]:\n    for input in inputs:\n        response, parsed, error = erx_json(\n            client,\n            text,\n            model=model,\n            response_format=input[\"response_format\"],\n            include_schema_in_system_prompt=input[\"include_schema_in_system_prompt\"],\n        )\n        success = error is None\n\n        response_format_type = input[\"response_format\"][\"type\"]\n        response_format_value = input[\"response_format\"].get(\"value\") is not None\n        response_format_json_schema = input[\"response_format\"].get(\"json_schema\") is not None\n        include_schema_in_system_prompt = input[\"include_schema_in_system_prompt\"]\n        results.append(\n            {\n                \"provider\": provider,\n                \"model\": model,\n                \"response_format_type\": response_format_type,\n                \"response_format_json_schema\": response_format_json_schema,\n                \"response_format_value\": response_format_value,\n                \"include_schema_in_system_prompt\": include_schema_in_system_prompt,\n                \"success\": success,\n                \"error\": error,\n            }\n        )\n\n\n\n\nCode\nimport pandas as pd\nfrom itables import show\n\ndef pretty_error(error):\n    if error is None:\n        return \"None\"\n    \n    parts = error.message.split(' - ', 1)\n    if len(parts) == 1:\n        return parts[0]\n\n    error_code, payload_str = parts\n    try:\n        error_message = eval(payload_str)['error']\n        if isinstance(error_message, str):\n            return error_message\n        elif 'message' in error_message:\n            error_message = error_message['message']\n        else:\n            error_message = payload_str\n    except Exception as e:\n        return error_code\n    \n    return f\"{error_code} - {error_message}\"\n\ndf = pd.json_normalize(results)\ndf['error'] = df['error'].map(pretty_error)\n\n\n\n\nCode\ntable = df[['provider', 'response_format_type', 'response_format_value', 'include_schema_in_system_prompt', 'success', 'error']]\nshow(table, paging=False)\n\n\n\n\n    \n      \n      provider\n      response_format_type\n      response_format_value\n      include_schema_in_system_prompt\n      success\n      error\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.2.4 from the internet...\n(need help?)"
  }
]